{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22f3792",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 18 – CS 189, Fall 2025</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import random as rn\n",
    "from torchinfo import summary\n",
    "from types import SimpleNamespace\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74831d",
   "metadata": {},
   "source": [
    "## Building CNN from Scratch in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce8672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e752ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "x_train_mnist = tv.datasets.MNIST(root='./data', train=True, \n",
    "                                          download=True, transform=mnist_tf)\n",
    "x_test_mnist = tv.datasets.MNIST(root='./data', train=False, \n",
    "                                         download=True, transform=mnist_tf)\n",
    "\n",
    "image_size = x_train_mnist[0][0].shape[1]\n",
    "\n",
    "\n",
    "print(f\"MNIST training data size: {len(x_train_mnist)} of size {image_size}x{image_size}\")\n",
    "print(f\"MNIST test data size: {len(x_test_mnist)} of size {image_size}x{image_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af06eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_mnist = DataLoader(x_train_mnist, batch_size=128, shuffle=True)\n",
    "test_loader_mnist  = DataLoader(x_test_mnist, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80379b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [x_train_mnist[i] for i in range(16)]  \n",
    "labels = [i for i in range(16)] \n",
    "fig, axes = plt.subplots(4, 4, figsize=(4, 4))\n",
    "for ax, img, lab in zip(axes.flatten(), imgs, labels):\n",
    "    ax.imshow(img[0].squeeze().numpy(), cmap='gray') \n",
    "    ax.set_title(str(lab))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750273b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 \n",
    "            nn.Conv2d(8, 64, 3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),               \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 256),  # Adjusted based on the output size of the features block\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "summary(model, input_size=(1, 1, 28, 28),\n",
    "        col_names=(\"input_size\",\"output_size\",\"num_params\",\"kernel_size\"),\n",
    "        depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ec8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, loss_fn):\n",
    "    model.train()\n",
    "    total, correct, running_loss = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()*xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total, correct, running_loss = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        running_loss += loss.item()*xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "if tv is not None:\n",
    "    model = SmallCNN().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = 4 \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tl, ta = train_one_epoch(model, train_loader_mnist, opt, loss_fn)\n",
    "        vl, va = evaluate(model, test_loader_mnist, loss_fn)\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(tl); history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(ta);  history['val_acc'].append(va)\n",
    "        print(f'E{epoch}: train_loss={tl:.4f} val_loss={vl:.4f} train_acc={ta:.3f} val_acc={va:.3f}')\n",
    "else:\n",
    "    print(\"torchvision not available. Skipping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e38ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if history and 'train_loss' in history and 'val_loss' in history:\n",
    "    plt.figure(); plt.plot(history['epoch'], history['train_loss']); plt.plot(history['epoch'], history['val_loss']); plt.legend(['train','val']); plt.title('Loss'); plt.xlabel('epoch'); plt.show()\n",
    "if history and 'train_acc' in history and 'val_acc' in history:\n",
    "    plt.figure(); plt.plot(history['epoch'], history['train_acc']); plt.plot(history['epoch'], history['val_acc']); plt.legend(['train','val']); plt.title('Accuracy'); plt.xlabel('epoch'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f64eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tv is not None:\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader_mnist:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_pred.extend(preds.cpu().numpy())\n",
    "            all_true.extend(yb.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    all_true = np.array(all_true)\n",
    "    all_pred = np.array(all_pred)\n",
    "    accuracy = (all_true == all_pred).sum() / len(all_true)\n",
    "    print(f\"Accuracy on test data: {accuracy:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_true, all_pred, labels=list(range(10)))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(range(10)), yticklabels=list(range(10)))\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"torchvision not available. Cannot evaluate predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f931a",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d56e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_w   = tv.models.AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "MODELS = {\n",
    "    \"alexnet\": SimpleNamespace(\n",
    "        ctor=lambda: tv.models.alexnet(weights=alex_w).to(device).eval(),\n",
    "        weights=alex_w,\n",
    "        act_layers=[\"features.0\", \"features.3\"],  # conv blocks\n",
    "        maxact_layer=\"features.10\",\n",
    "        arch=\"alexnet\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7718230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, weights):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return weights.transforms()(img).unsqueeze(0).to(device)\n",
    "\n",
    "def show(tensor, title=None):\n",
    "    if tensor.ndim == 4:\n",
    "        grid = make_grid(tensor, nrow=int(np.ceil(np.sqrt(tensor.size(0)))))\n",
    "        arr = grid.permute(1,2,0).detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = tensor.permute(1,2,0).detach().cpu().numpy()\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.axis('off')\n",
    "    if title: plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Robust per-item normalization (avoids tuple-dim min/max issues)\n",
    "def _norm_per_item(t):\n",
    "    # t shape: [N, ...]\n",
    "    if hasattr(torch, \"amin\"):\n",
    "        tmin = torch.amin(t, dim=tuple(range(1, t.ndim)), keepdim=True)\n",
    "        tmax = torch.amax(t, dim=tuple(range(1, t.ndim)), keepdim=True)\n",
    "    else:\n",
    "        flat = t.view(t.size(0), -1)\n",
    "        tmin = flat.min(dim=1, keepdim=True)[0].view(-1, *([1]*(t.ndim-1)))\n",
    "        tmax = flat.max(dim=1, keepdim=True)[0].view(-1, *([1]*(t.ndim-1)))\n",
    "    return (t - tmin) / (tmax - tmin + 1e-8)\n",
    "\n",
    "# Helper: resolve \"features.23\" dotted path to a module\n",
    "def resolve_module(root, name):\n",
    "    mod = root\n",
    "    for part in name.split('.'):\n",
    "        if part.isdigit():\n",
    "            mod = mod[int(part)]\n",
    "        else:\n",
    "            mod = getattr(mod, part)\n",
    "    return mod\n",
    "\n",
    "def first_conv_module(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            return m\n",
    "    raise RuntimeError(\"No Conv2d found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f5ae164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_first_layer_filters(model, max_filters=64, label=\"model\"):\n",
    "    conv1 = None\n",
    "    # Try common entry points first\n",
    "    for attr in [\"conv1\", \"features\"]:\n",
    "        if hasattr(model, attr):\n",
    "            m = getattr(model, attr)\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                conv1 = m\n",
    "                break\n",
    "            # If Sequential, first layer likely Conv2d\n",
    "            if isinstance(m, nn.Sequential):\n",
    "                for x in m:\n",
    "                    if isinstance(x, nn.Conv2d):\n",
    "                        conv1 = x; break\n",
    "        if conv1 is not None: break\n",
    "    if conv1 is None:\n",
    "        conv1 = first_conv_module(model)\n",
    "\n",
    "    w = conv1.weight.data.clone().cpu()  # [out, in, k, k]\n",
    "    w = _norm_per_item(w)\n",
    "    show(w[:max_filters], title=f\"{label}: first-layer conv filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2094b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(model, img_tensor, layer_names, label=\"model\"):\n",
    "    feats, hooks = {}, []\n",
    "    def hook(name): \n",
    "        return lambda m, i, o: feats.setdefault(name, o.detach().cpu())\n",
    "    # Register hooks\n",
    "    for name in layer_names:\n",
    "        try:\n",
    "            module = resolve_module(model, name)\n",
    "            hooks.append(module.register_forward_hook(hook(name)))\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] could not hook '{name}': {e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(img_tensor)\n",
    "    for h in hooks: h.remove()\n",
    "\n",
    "    for name, feat in feats.items():\n",
    "        fmap = feat[0]                    \n",
    "        if fmap.ndim != 3:\n",
    "            print(f\"[info] {label}:{name} is non-spatial (shape {feat.shape}), skipping grid\")\n",
    "            continue\n",
    "        C = min(64, fmap.size(0))\n",
    "        fm = fmap[:C]\n",
    "        fm = _norm_per_item(fm.unsqueeze(1)).squeeze(1)\n",
    "        show(fm.unsqueeze(1), title=f\"{label}: activations @ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a12712",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_probs(model, x):\n",
    "    logits = model(x)\n",
    "    return F.softmax(logits, dim=1)\n",
    "\n",
    "def _resize_heat_with_torch(heat, H, W):\n",
    "    t = torch.from_numpy(heat)[None, None]\n",
    "    t = F.interpolate(t.float(), size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "    return t[0,0].numpy()\n",
    "\n",
    "def occlusion_heatmap(model, img_tensor, idx_to_label=None, target_class=None, patch=32, stride=16, baseline=0.0, label=\"model\"):\n",
    "    model.eval()\n",
    "    x = img_tensor.clone()\n",
    "    probs = predict_probs(model, x)[0]\n",
    "    if target_class is None:\n",
    "        target_class = probs.argmax().item()\n",
    "    base_p = probs[target_class].item()\n",
    "\n",
    "    _, _, H, W = x.shape\n",
    "    heat = np.zeros(((H - patch)//stride + 1, (W - patch)//stride + 1), dtype=np.float32)\n",
    "\n",
    "    for i, y in enumerate(range(0, H - patch + 1, stride)):\n",
    "        for j, z in enumerate(range(0, W - patch + 1, stride)):\n",
    "            x_ = x.clone()\n",
    "            x_[:,:, y:y+patch, z:z+patch] = baseline\n",
    "            p = predict_probs(model, x_)[0, target_class].item()\n",
    "            heat[i, j] = base_p - p\n",
    "\n",
    "    heat_resized = _resize_heat_with_torch(heat, H, W)\n",
    "    # quick unnormalize for show (using Imagenet stats)\n",
    "    im = x[0].detach().cpu()\n",
    "    im = (im * torch.tensor([0.229,0.224,0.225])[:,None,None] + torch.tensor([0.485,0.456,0.406])[:,None,None]).permute(1,2,0).numpy()\n",
    "    plt.figure(figsize=(6,6)); plt.imshow(np.clip(im,0,1)); plt.imshow(heat_resized, alpha=0.5); plt.axis('off')\n",
    "    if idx_to_label:\n",
    "        tname = idx_to_label[target_class]\n",
    "    else:\n",
    "        tname = str(target_class)\n",
    "    plt.title(f\"{label}: occlusion (target='{tname}', base p={base_p:.3f})\")\n",
    "    plt.show()\n",
    "    return heat_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "109a31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(model, img_tensor, target_class=None, label=\"model\"):\n",
    "    model.eval()\n",
    "    x = img_tensor.clone().requires_grad_(True)\n",
    "    logits = model(x)\n",
    "    if target_class is None:\n",
    "        target_class = logits.argmax(dim=1).item()\n",
    "    loss = logits[0, target_class]\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    g = x.grad.detach()[0]               # [3,H,W]\n",
    "    sal = g.abs().max(dim=0)[0]          # [H,W]\n",
    "    sal = (sal - sal.min())/(sal.max()-sal.min()+1e-8)\n",
    "    plt.figure(figsize=(6,6)); plt.imshow(sal.cpu(), cmap='gray'); plt.axis('off'); plt.title(f\"{label}: saliency\")\n",
    "    plt.show()\n",
    "    return sal\n",
    "\n",
    "class GuidedBackpropReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        self.saved = x\n",
    "        return F.relu(x)\n",
    "    def backward_hook(self, module, grad_in, grad_out):\n",
    "        positive_grad = torch.clamp(grad_out[0], min=0.0)\n",
    "        positive_mask = (self.saved > 0).float()\n",
    "        return (positive_grad * positive_mask,)\n",
    "\n",
    "def guided_backprop(model_ctor, weights, img_tensor, target_class=None, label=\"model\"):\n",
    "    # Create a fresh copy to freely patch ReLUs\n",
    "    gb_model = model_ctor().to(device).eval()\n",
    "    # Swap all ReLUs\n",
    "    relus = []\n",
    "    for name, module in gb_model.named_modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            relu = GuidedBackpropReLU()\n",
    "            relus.append(relu)\n",
    "            parent = gb_model\n",
    "            *parents, leaf = name.split('.')\n",
    "            for p in parents:\n",
    "                parent = getattr(parent, p)\n",
    "            setattr(parent, leaf, relu)\n",
    "    x = img_tensor.clone().requires_grad_(True)\n",
    "    logits = gb_model(x)\n",
    "    if target_class is None:\n",
    "        target_class = logits.argmax(dim=1).item()\n",
    "    loss = logits[0, target_class]\n",
    "    gb_model.zero_grad()\n",
    "    hooks = [relu.register_full_backward_hook(relu.backward_hook) for relu in relus]\n",
    "    loss.backward()\n",
    "    for h in hooks: h.remove()\n",
    "\n",
    "    g = x.grad.detach()[0]\n",
    "    g = (g - g.min())/(g.max()-g.min()+1e-8)\n",
    "    g = g.permute(1,2,0).cpu().numpy()\n",
    "    plt.figure(figsize=(6,6)); plt.imshow(g); plt.axis('off'); plt.title(f\"{label}: guided backprop\")\n",
    "    plt.show()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "111d7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatExtractor(nn.Module):\n",
    "    \"\"\"Return a fixed-dim feature vector (penultimate-ish) for each arch.\"\"\"\n",
    "    def __init__(self, model, arch):\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        self.model = model\n",
    "        if arch == \"resnet\":\n",
    "            # body up to layer4 GAP\n",
    "            self.body = nn.Sequential(\n",
    "                model.conv1, model.bn1, model.relu, model.maxpool,\n",
    "                model.layer1, model.layer2, model.layer3, model.layer4,\n",
    "                nn.AdaptiveAvgPool2d((1,1))\n",
    "            )\n",
    "            self.out_dim = model.fc.in_features\n",
    "        elif arch == \"vgg\" or arch == \"alexnet\":\n",
    "            self.features = model.features\n",
    "            self.pool = nn.AdaptiveAvgPool2d((7,7))  # match VGG/Alex input to classifier\n",
    "            # classifier: take everything except final Linear\n",
    "            self.prefix = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "            # out_dim is the in_features of final Linear\n",
    "            last_linear = list(model.classifier.children())[-1]\n",
    "            self.out_dim = last_linear.in_features\n",
    "        else:\n",
    "            raise ValueError(\"Unknown arch\")\n",
    "    def forward(self, x):\n",
    "        if self.arch == \"resnet\":\n",
    "            x = self.body(x).flatten(1)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.features(x)\n",
    "            x = self.pool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.prefix(x)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc70de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_activating_images(model, dataset, layer_name, topk=16, label=\"model\"):\n",
    "    target = resolve_module(model, layer_name)\n",
    "    acts = []\n",
    "    imgs_cache = []\n",
    "    def fhook(m, i, o):\n",
    "        if o.ndim == 4:\n",
    "            a = o.detach().cpu().mean(dim=(2,3))  # GAP over H,W → [B, C]\n",
    "        else:\n",
    "            a = o.detach().cpu()\n",
    "        acts.append(a)\n",
    "    h = target.register_forward_hook(fhook)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            imgs_cache.append(xb)\n",
    "            _ = model(xb.to(device))\n",
    "    h.remove()\n",
    "    A = torch.cat(acts, 0).numpy()      # [N, C]\n",
    "    imgs_cache = torch.cat(imgs_cache, 0)\n",
    "    # Choose an arbitrary channel to inspect (customize this)\n",
    "    channel = min(5, A.shape[1]-1)\n",
    "    idxs = np.argsort(-A[:, channel])[:topk]\n",
    "    grid = imgs_cache[idxs]\n",
    "    # unnormalize for viewing (ImageNet stats)\n",
    "    grid = grid*torch.tensor([0.229,0.224,0.225])[None,:,None,None] + torch.tensor([0.485,0.456,0.406])[None,:,None,None]\n",
    "    grid = grid.clamp(0,1)\n",
    "    show(grid, title=f\"{label}: top-{topk} images for channel {channel} @ {layer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55808969",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"pishi.png\" \n",
    "from PIL import Image\n",
    "\n",
    "models = {}\n",
    "for name, cfg in MODELS.items():\n",
    "    m = cfg.ctor()\n",
    "    models[name] = SimpleNamespace(\n",
    "        model=m, weights=cfg.weights, act_layers=cfg.act_layers,\n",
    "        maxact_layer=cfg.maxact_layer, arch=cfg.arch,\n",
    "        idx_to_label=cfg.weights.meta.get(\"categories\", None)\n",
    "    )\n",
    "\n",
    "# Ensure the input image is resized to 224x224\n",
    "images = {name: load_image(img_path, cfg.weights) for name, cfg in models.items()}\n",
    "for name, img in images.items():\n",
    "    assert img.shape[-2:] == (224, 224), f\"Image for model {name} is not resized to 224x224\"\n",
    "\n",
    "# 1) First-layer filters comparison\n",
    "for name, cfg in models.items():\n",
    "    visualize_first_layer_filters(cfg.model, max_filters=64, label=name)\n",
    "\n",
    "# 2) Activation maps at key layers\n",
    "for name, cfg in models.items():\n",
    "    visualize_activations(cfg.model, images[name], cfg.act_layers, label=name)\n",
    "\n",
    "# 3) Occlusion sensitivity (same target class per model by default)\n",
    "for name, cfg in models.items():\n",
    "    _ = occlusion_heatmap(cfg.model, images[name], idx_to_label=cfg.idx_to_label, patch=32, stride=16, label=name)\n",
    "\n",
    "# 4) Saliency and Guided Backprop\n",
    "for name, cfg in models.items():\n",
    "    _ = saliency_map(cfg.model, images[name], label=name)\n",
    "    _ = guided_backprop(MODELS[name].ctor, cfg.weights, images[name], label=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc1f4e",
   "metadata": {},
   "source": [
    "## Transfer Learning on CIFAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd4806",
   "metadata": {},
   "source": [
    "### CIFAR-100 Dataset\n",
    "\n",
    "The CIFAR-100 dataset is a widely used benchmark in machine learning and computer vision, particularly for image classification tasks. It is an extension of the CIFAR-10 dataset, offering more classes and finer granularity. Below is a detailed breakdown of its key features:\n",
    "\n",
    "#### 1. Dataset Overview\n",
    "- **Name**: CIFAR-100 (Canadian Institute For Advanced Research)\n",
    "- **Number of Classes**: 100\n",
    "- **Number of Images**: 60,000\n",
    "    - 50,000 training images\n",
    "    - 10,000 test images\n",
    "- **Image Size**: 32x32 pixels\n",
    "- **Channels**: 3 (RGB, meaning the images are in color)\n",
    "\n",
    "#### 2. Labels\n",
    "Each image in the dataset is associated with two types of labels:\n",
    "- **Fine Label**: Specifies one of the 100 detailed classes (e.g., \"apple\", \"dolphin\", \"castle\").\n",
    "- **Coarse Label**: Groups the fine labels into 20 broader superclasses (e.g., \"fruit and vegetables\", \"aquatic mammals\", \"buildings\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load training data\n",
    "cifar_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_cifar100 = tv.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=cifar_tf)\n",
    "\n",
    "X_cifar = data_cifar100.data  # numpy array (50000, 32, 32, 3)\n",
    "y_cifar = np.array(data_cifar100.targets)  # numpy array (50000,)\n",
    "\n",
    "print('X shape: ', X_cifar.shape)\n",
    "print('y shape:', y_cifar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326408ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 4)\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(4):\n",
    "        l = rn.randint(0, len(y_cifar))\n",
    "        ax[i, j].imshow(X_cifar[l])\n",
    "        ax[i, j].set_title('Label: ' + str(y_cifar[l]))\n",
    "        ax[i, j].grid(False)\n",
    "        ax[i, j].set_xticks([])\n",
    "        ax[i, j].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cifar = X_cifar / 255.0\n",
    "\n",
    "# Convert to PyTorch tensors and reshape to (N, C, H, W) format\n",
    "X_cifar_tensor = torch.FloatTensor(X_cifar).permute(0, 3, 1, 2)  # (50000, 3, 32, 32)\n",
    "y_cifar_tensor = torch.LongTensor(y_cifar)\n",
    "\n",
    "print('X_tensor shape: ', X_cifar_tensor.shape)\n",
    "print('y_tensor shape:', y_cifar_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc8ce347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and split into train/val\n",
    "dataset_cifar = TensorDataset(X_cifar_tensor, y_cifar_tensor)\n",
    "train_size_cifar_100 = int(0.9 * len(dataset_cifar))\n",
    "val_size_cifar = len(dataset_cifar) - train_size_cifar_100\n",
    "train_dataset_cifar, val_dataset_cifar = random_split(dataset_cifar, [train_size_cifar_100, val_size_cifar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef222a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 200\n",
    "train_loader_cifar = DataLoader(train_dataset_cifar, batch_size=batch_size, shuffle=True)\n",
    "val_loader_cifar = DataLoader(val_dataset_cifar, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e919530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Demo(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(CNN_Demo, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, \n",
    "                              kernel_size=3, padding=1)  # 'same' padding\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, \n",
    "                              kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 200)  # After 2 pooling layers: 32->16->8\n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2daa2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "model = CNN_Demo(num_classes=100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af45843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "summary(model, input_size=(1, 3, 32, 32),\n",
    "        col_names=(\"input_size\",\"output_size\",\"num_params\",\"kernel_size\"),\n",
    "        depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2aa387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader_cifar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss / train_total\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_cifar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968160fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='val')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='train')\n",
    "plt.plot(history['val_acc'], label='val')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ab45483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "# Load pretrained VGG16\n",
    "base_model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# VGG16 expects 224x224 images, but CIFAR-100 is 32x32\n",
    "# We'll modify the architecture to work with 32x32 images\n",
    "\n",
    "class VGG16Transfer(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(VGG16Transfer, self).__init__()\n",
    "        \n",
    "        # Use VGG16 features (convolutional layers)\n",
    "        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        self.features = vgg.features\n",
    "        \n",
    "        # Modify classifier for CIFAR-100\n",
    "        # After VGG features with 32x32 input, we get 1x1x512\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512, 200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transfer learning model\n",
    "transfer_model = VGG16Transfer(num_classes=100).to(device)\n",
    "\n",
    "# Freeze the feature extraction layers\n",
    "for param in transfer_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train the classifier layers\n",
    "optimizer_transfer = optim.Adagrad(transfer_model.parameters(), lr=0.001)\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(transfer_model, input_size=(1, 3, 32, 32), \n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"), \n",
    "    depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c40a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_transfer = 2\n",
    "history_transfer = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(epochs_transfer):\n",
    "    # Training phase\n",
    "    transfer_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader_cifar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer_transfer.zero_grad()\n",
    "        outputs = transfer_model(inputs)\n",
    "        loss = criterion_transfer(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_transfer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss / train_total\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    transfer_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_cifar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = transfer_model(inputs)\n",
    "            loss = criterion_transfer(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    history_transfer['train_loss'].append(train_loss)\n",
    "    history_transfer['train_acc'].append(train_acc)\n",
    "    history_transfer['val_loss'].append(val_loss)\n",
    "    history_transfer['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs_transfer}] - '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9737450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transfer learning results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_transfer['train_loss'], label='train')\n",
    "plt.plot(history_transfer['val_loss'], label='val')\n",
    "plt.title('Transfer Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_transfer['train_acc'], label='train')\n",
    "plt.plot(history_transfer['val_acc'], label='val')\n",
    "plt.title('Transfer Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4440f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random image\n",
    "img_idx = rn.randint(0, len(X_cifar_tensor))\n",
    "img = X_cifar_tensor[img_idx:img_idx+1].to(device)\n",
    "\n",
    "# Display original image\n",
    "plt.imshow(X_cifar[img_idx])\n",
    "plt.grid(False)\n",
    "plt.title('Original Image')\n",
    "plt.show()\n",
    "\n",
    "# Hook to capture intermediate layer outputs\n",
    "activation = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks for convolutional layers\n",
    "transfer_model.features[2].register_forward_hook(get_activation('conv1'))\n",
    "transfer_model.features[7].register_forward_hook(get_activation('conv2'))\n",
    "transfer_model.features[12].register_forward_hook(get_activation('conv3'))\n",
    "transfer_model.features[19].register_forward_hook(get_activation('conv4'))\n",
    "transfer_model.features[26].register_forward_hook(get_activation('conv5'))\n",
    "\n",
    "# Forward pass\n",
    "transfer_model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = transfer_model(img)\n",
    "\n",
    "# Visualize feature maps\n",
    "layer_names = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5']\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    if layer_name in activation:\n",
    "        feature_maps = activation[layer_name].cpu().numpy()[0]  # (num_channels, H, W)\n",
    "        num_filters = min(64, feature_maps.shape[0])\n",
    "        \n",
    "        fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "        fig.suptitle(f'Feature Maps from {layer_name}', fontsize=16)\n",
    "        \n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                idx = i * 8 + j\n",
    "                if idx < num_filters:\n",
    "                    axes[i, j].imshow(feature_maps[idx], cmap='viridis')\n",
    "                    axes[i, j].set_title(f'K{idx}', fontsize=8)\n",
    "                axes[i, j].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0edb6",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize the MNIST images back to [0, 1]\n",
    "x_train_mnist_np = np.array([x[0].numpy() for x in x_train_mnist]) * 0.3081 + 0.1307\n",
    "x_test_mnist_np = np.array([x[0].numpy() for x in x_test_mnist]) * 0.3081 + 0.1307\n",
    "\n",
    "# Generate corrupted MNIST images by adding noise with normal distribution\n",
    "noise = np.random.normal(loc=0.0, scale=0.5, size=x_train_mnist_np.shape)\n",
    "x_train_noisy = x_train_mnist_np + noise\n",
    "noise = np.random.normal(loc=0.0, scale=0.5, size=x_test_mnist_np.shape)\n",
    "x_test_noisy = x_test_mnist_np + noise\n",
    "\n",
    "# Clip values to [0, 1]\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Display the first 25 corrupted and original images\n",
    "rows, cols = 5, 5\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([x_train_mnist_np[:num], x_train_noisy[:num]])\n",
    "imgs = imgs.reshape((rows * 2, cols, image_size, image_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 2, -1, image_size, image_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, Corrupted Input: bottom rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "# plt.savefig('original_vs_noisy.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04014c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors with shape (N, C, H, W)\n",
    "x_train_tensor       = torch.from_numpy(x_train_mnist_np).float()\n",
    "x_train_noisy_tensor = torch.from_numpy(x_train_noisy).float()\n",
    "x_test_tensor        = torch.from_numpy(x_test_mnist_np).float()\n",
    "x_test_noisy_tensor  = torch.from_numpy(x_test_noisy).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c7b52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(x_train_noisy_tensor, x_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_noisy_tensor, x_test_tensor)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "092d756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "input_shape = (1, image_size, image_size)  # (C, H, W) format for PyTorch\n",
    "kernel_size = 3\n",
    "latent_dim = 16\n",
    "# Encoder/Decoder number of CNN layers and filters per layer\n",
    "layer_filters = [32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16, layer_filters=[32, 64], kernel_size=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Stack of Conv2D blocks\n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        for filters in layer_filters:\n",
    "            layers.append(nn.Conv2d(in_channels, filters, kernel_size, \n",
    "                                   stride=2, padding=1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            in_channels = filters\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Calculate the flattened size after convolutions\n",
    "        # After each stride=2 conv with padding=1: size -> (size + 1) // 2\n",
    "        # 28 -> 14 -> 7\n",
    "        self.flatten_size = layer_filters[-1] * 7 * 7\n",
    "        \n",
    "        # Latent vector layer\n",
    "        self.fc = nn.Linear(self.flatten_size, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # Get shape before flattening (needed for decoder)\n",
    "        self.shape_before_flatten = x.shape\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Instantiate Encoder\n",
    "encoder = Encoder(latent_dim=latent_dim, layer_filters=layer_filters, \n",
    "                 kernel_size=kernel_size).to(device)\n",
    "\n",
    "print(\"Encoder Architecture:\")\n",
    "# Ensure input_shape matches the expected dimensions (batch_size, channels, height, width)\n",
    "input_shape = (1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
    "summary(encoder, input_size=(1, 1, 28, 28), col_names=(\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16, layer_filters=[32, 64], kernel_size=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Calculate the size after encoder\n",
    "        # 28 -> 14 -> 7 (shape: [batch, 64, 7, 7])\n",
    "        self.shape_h = 7\n",
    "        self.shape_w = 7\n",
    "        self.shape_c = layer_filters[-1]\n",
    "        \n",
    "        # Dense layer to reshape from latent vector\n",
    "        self.fc = nn.Linear(latent_dim, self.shape_c * self.shape_h * self.shape_w)\n",
    "        \n",
    "        # Stack of Transposed Conv2D blocks (reverse order)\n",
    "        layers = []\n",
    "        reversed_filters = layer_filters[::-1]\n",
    "        \n",
    "        for i in range(len(reversed_filters)):\n",
    "            in_channels = reversed_filters[i]\n",
    "            out_channels = reversed_filters[i+1] if i+1 < len(reversed_filters) else 1\n",
    "            \n",
    "            if i < len(reversed_filters) - 1:\n",
    "                # Intermediate layers with ReLU\n",
    "                layers.append(nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                                                kernel_size, stride=2, \n",
    "                                                padding=1, output_padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "            else:\n",
    "                # Last transposed conv to get back to original size\n",
    "                layers.append(nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                                                kernel_size, stride=2, \n",
    "                                                padding=1, output_padding=1))\n",
    "        \n",
    "        # Final conv to ensure exact output size and add sigmoid\n",
    "        layers.append(nn.Conv2d(1, 1, kernel_size, padding=1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.deconv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        x = self.fc(latent)\n",
    "        x = x.view(-1, self.shape_c, self.shape_h, self.shape_w)\n",
    "        x = self.deconv_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate Decoder\n",
    "decoder = Decoder(latent_dim=latent_dim, layer_filters=layer_filters, \n",
    "                 kernel_size=kernel_size).to(device)\n",
    "\n",
    "print(\"\\nDecoder Architecture:\")\n",
    "summary(decoder, input_size=(latent_dim,), col_names=(\"input_size\", \"output_size\", \"num_params\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b922508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "# Instantiate Autoencoder\n",
    "autoencoder = Autoencoder(encoder, decoder).to(device)\n",
    "\n",
    "print(\"\\nComplete Autoencoder:\")\n",
    "summary(autoencoder, input_size=(1, 1, 28, 28), col_names=(\"input_size\", \"output_size\", \"num_params\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3815ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters())\n",
    "\n",
    "# Training function\n",
    "def train_autoencoder(model, train_loader, test_loader, criterion, optimizer, epochs=2):\n",
    "    \"\"\"Train the autoencoder\"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for noisy_imgs, clean_imgs in train_loader:\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(noisy_imgs)\n",
    "            loss = criterion(outputs, clean_imgs)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * noisy_imgs.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for noisy_imgs, clean_imgs in test_loader:\n",
    "                noisy_imgs = noisy_imgs.to(device)\n",
    "                clean_imgs = clean_imgs.to(device)\n",
    "                \n",
    "                outputs = model(noisy_imgs)\n",
    "                loss = criterion(outputs, clean_imgs)\n",
    "                \n",
    "                val_loss += loss.item() * noisy_imgs.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(test_loader.dataset)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}] - '\n",
    "              f'Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the autoencoder\n",
    "print(\"\\nTraining autoencoder...\")\n",
    "history = train_autoencoder(autoencoder, train_loader, test_loader, \n",
    "                           criterion, optimizer, epochs=2)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig('training_history.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Autoencoder output from corrupted test images\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    # Get predictions for first batch\n",
    "    x_test_noisy_batch = x_test_noisy_tensor.to(device)\n",
    "    x_decoded = autoencoder(x_test_noisy_batch).cpu()\n",
    "\n",
    "# Convert back to numpy for visualization\n",
    "x_decoded = x_decoded.numpy()  # Already has the correct dimensions\n",
    "\n",
    "# Display the first 25 images: original, corrupted, and denoised\n",
    "rows, cols = 5, 5\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([x_test_mnist_np[:num], x_test_noisy[:num], x_decoded[:num]])\n",
    "imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "imgs = (imgs * 255).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, '\n",
    "          'Corrupted Input: middle rows, '\n",
    "          'Denoised Output: bottom rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "# plt.savefig('denoising_results.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
