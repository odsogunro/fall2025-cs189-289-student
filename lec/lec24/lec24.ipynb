{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22f3792",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 24 â€“ CS 189, Fall 2025</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import random as rn\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce8672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e752ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "x_train_mnist = tv.datasets.MNIST(root='./data', train=True, \n",
    "                                          download=True, transform=mnist_tf)\n",
    "x_test_mnist = tv.datasets.MNIST(root='./data', train=False, \n",
    "                                         download=True, transform=mnist_tf)\n",
    "\n",
    "image_size = x_train_mnist[0][0].shape[1]\n",
    "\n",
    "\n",
    "print(f\"MNIST training data size: {len(x_train_mnist)} of size {image_size}x{image_size}\")\n",
    "print(f\"MNIST test data size: {len(x_test_mnist)} of size {image_size}x{image_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80379b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [x_train_mnist[i] for i in range(16)]  \n",
    "labels = [i for i in range(16)] \n",
    "fig, axes = plt.subplots(4, 4, figsize=(4, 4))\n",
    "for ax, img, lab in zip(axes.flatten(), imgs, labels):\n",
    "    ax.imshow(img[0].squeeze().numpy(), cmap='gray') \n",
    "    ax.set_title(str(lab))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0edb6",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize the MNIST images back to [0, 1]\n",
    "x_train_mnist_np = np.array([x[0].numpy() for x in x_train_mnist]) * 0.3081 + 0.1307\n",
    "x_test_mnist_np = np.array([x[0].numpy() for x in x_test_mnist]) * 0.3081 + 0.1307\n",
    "\n",
    "# Generate corrupted MNIST images by adding noise with normal distribution\n",
    "noise = np.random.normal(loc=0.0, scale=0.5, size=x_train_mnist_np.shape)\n",
    "x_train_noisy = x_train_mnist_np + noise\n",
    "noise = np.random.normal(loc=0.0, scale=0.5, size=x_test_mnist_np.shape)\n",
    "x_test_noisy = x_test_mnist_np + noise\n",
    "\n",
    "# Clip values to [0, 1]\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Display the first 25 corrupted and original images\n",
    "rows, cols = 5, 5\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([x_train_mnist_np[:num], x_train_noisy[:num]])\n",
    "imgs = imgs.reshape((rows * 2, cols, image_size, image_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 2, -1, image_size, image_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, Corrupted Input: bottom rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "# plt.savefig('original_vs_noisy.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04014c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors with shape (N, C, H, W)\n",
    "x_train_tensor       = torch.from_numpy(x_train_mnist_np).float()\n",
    "x_train_noisy_tensor = torch.from_numpy(x_train_noisy).float()\n",
    "x_test_tensor        = torch.from_numpy(x_test_mnist_np).float()\n",
    "x_test_noisy_tensor  = torch.from_numpy(x_test_noisy).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7b52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(x_train_noisy_tensor, x_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_noisy_tensor, x_test_tensor)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092d756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "input_shape = (1, image_size, image_size)  # (C, H, W) format for PyTorch\n",
    "kernel_size = 3\n",
    "latent_dim = 16\n",
    "# Encoder/Decoder number of CNN layers and filters per layer\n",
    "layer_filters = [32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16, layer_filters=[32, 64], kernel_size=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Stack of Conv2D blocks\n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        for filters in layer_filters:\n",
    "            layers.append(nn.Conv2d(in_channels, filters, kernel_size, \n",
    "                                   stride=2, padding=1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            in_channels = filters\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Calculate the flattened size after convolutions\n",
    "        # After each stride=2 conv with padding=1: size -> (size + 1) // 2\n",
    "        # 28 -> 14 -> 7\n",
    "        self.flatten_size = layer_filters[-1] * 7 * 7\n",
    "        \n",
    "        # Latent vector layer\n",
    "        self.fc = nn.Linear(self.flatten_size, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # Get shape before flattening (needed for decoder)\n",
    "        self.shape_before_flatten = x.shape\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Instantiate Encoder\n",
    "encoder = Encoder(latent_dim=latent_dim, layer_filters=layer_filters, \n",
    "                 kernel_size=kernel_size).to(device)\n",
    "\n",
    "print(\"Encoder Architecture:\")\n",
    "# Ensure input_shape matches the expected dimensions (batch_size, channels, height, width)\n",
    "input_shape = (1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
    "summary(encoder, input_size=(1, 1, 28, 28), col_names=(\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16, layer_filters=[32, 64], kernel_size=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Calculate the size after encoder\n",
    "        # 28 -> 14 -> 7 (shape: [batch, 64, 7, 7])\n",
    "        self.shape_h = 7\n",
    "        self.shape_w = 7\n",
    "        self.shape_c = layer_filters[-1]\n",
    "        \n",
    "        # Dense layer to reshape from latent vector\n",
    "        self.fc = nn.Linear(latent_dim, self.shape_c * self.shape_h * self.shape_w)\n",
    "        \n",
    "        # Stack of Transposed Conv2D blocks (reverse order)\n",
    "        layers = []\n",
    "        reversed_filters = layer_filters[::-1]\n",
    "        \n",
    "        for i in range(len(reversed_filters)):\n",
    "            in_channels = reversed_filters[i]\n",
    "            out_channels = reversed_filters[i+1] if i+1 < len(reversed_filters) else 1\n",
    "            \n",
    "            if i < len(reversed_filters) - 1:\n",
    "                # Intermediate layers with ReLU\n",
    "                layers.append(nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                                                kernel_size, stride=2, \n",
    "                                                padding=1, output_padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "            else:\n",
    "                # Last transposed conv to get back to original size\n",
    "                layers.append(nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                                                kernel_size, stride=2, \n",
    "                                                padding=1, output_padding=1))\n",
    "        \n",
    "        # Final conv to ensure exact output size and add sigmoid\n",
    "        layers.append(nn.Conv2d(1, 1, kernel_size, padding=1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.deconv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        x = self.fc(latent)\n",
    "        x = x.view(-1, self.shape_c, self.shape_h, self.shape_w)\n",
    "        x = self.deconv_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate Decoder\n",
    "decoder = Decoder(latent_dim=latent_dim, layer_filters=layer_filters, \n",
    "                 kernel_size=kernel_size).to(device)\n",
    "\n",
    "print(\"\\nDecoder Architecture:\")\n",
    "summary(decoder, input_size=(latent_dim,), col_names=(\"input_size\", \"output_size\", \"num_params\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b922508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "# Instantiate Autoencoder\n",
    "autoencoder = Autoencoder(encoder, decoder).to(device)\n",
    "\n",
    "print(\"\\nComplete Autoencoder:\")\n",
    "summary(autoencoder, input_size=(1, 1, 28, 28), col_names=(\"input_size\", \"output_size\", \"num_params\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3815ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters())\n",
    "\n",
    "# Training function\n",
    "def train_autoencoder(model, train_loader, test_loader, criterion, optimizer, epochs=2):\n",
    "    \"\"\"Train the autoencoder\"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for noisy_imgs, clean_imgs in train_loader:\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(noisy_imgs)\n",
    "            loss = criterion(outputs, clean_imgs)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * noisy_imgs.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for noisy_imgs, clean_imgs in test_loader:\n",
    "                noisy_imgs = noisy_imgs.to(device)\n",
    "                clean_imgs = clean_imgs.to(device)\n",
    "                \n",
    "                outputs = model(noisy_imgs)\n",
    "                loss = criterion(outputs, clean_imgs)\n",
    "                \n",
    "                val_loss += loss.item() * noisy_imgs.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(test_loader.dataset)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}] - '\n",
    "              f'Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the autoencoder\n",
    "print(\"\\nTraining autoencoder...\")\n",
    "history = train_autoencoder(autoencoder, train_loader, test_loader, \n",
    "                           criterion, optimizer, epochs=2)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig('training_history.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Autoencoder output from corrupted test images\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    # Get predictions for first batch\n",
    "    x_test_noisy_batch = x_test_noisy_tensor.to(device)\n",
    "    x_decoded = autoencoder(x_test_noisy_batch).cpu()\n",
    "\n",
    "# Convert back to numpy for visualization\n",
    "x_decoded = x_decoded.numpy()  # Already has the correct dimensions\n",
    "\n",
    "# Display the first 25 images: original, corrupted, and denoised\n",
    "rows, cols = 5, 5\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([x_test_mnist_np[:num], x_test_noisy[:num], x_decoded[:num]])\n",
    "imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "imgs = (imgs * 255).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, '\n",
    "          'Corrupted Input: middle rows, '\n",
    "          'Denoised Output: bottom rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "# plt.savefig('denoising_results.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
