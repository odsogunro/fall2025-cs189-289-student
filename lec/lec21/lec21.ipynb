{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f087764",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 21 â€“ CS 189, Fall 2025</h1> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba31153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598dca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# device = \"cuda\" # Colab\n",
    "device = \"mps\" # Mac with M1/M2\n",
    "# device = \"cpu\" # Local CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"vscode\" # VSCode\n",
    "# pio.renderers.default = \"colab\" # Colab support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcbc6d5",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Sinusoidal Embeddings</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 6\n",
    "n = 16\n",
    "L = 1000\n",
    "torch.arange(0, D, 2, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98280f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_pe(N, D, L=1000):\n",
    "    pe = torch.zeros(N, D)\n",
    "    div_term = L ** (2 * torch.arange(0, D, 2, dtype=torch.float) / D)\n",
    "    pe[:, 0::2] = torch.sin(torch.arange(N, dtype=torch.float).unsqueeze(1) / div_term)\n",
    "    pe[:, 1::2] = torch.cos(torch.arange(N, dtype=torch.float).unsqueeze(1) / div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e70dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64; D = 128; L = 10\n",
    "pe = sinusoidal_pe(n, D, L)\n",
    "px.imshow(pe.cpu().numpy().T, aspect='auto', color_continuous_scale='RdBu_r',\n",
    "          width = 1100, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pe @ pe.T\n",
    "fig = px.imshow(dist.cpu().numpy(), color_continuous_scale='Viridis',\n",
    "                width=700, height=700,\n",
    "                title='Dot Product of Positional Encodings')\n",
    "fig.show()\n",
    "px.line(x=np.arange(0, n), y=dist[10].cpu().numpy(), width=800, height=400,\n",
    "        title='Dot Product of Positional Encodings for Position 200')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b97fe8",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2  class=\"cal cal-h2\">Tokenization</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc396c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the Qwen tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\")\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4889576",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213ff8b",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2  class=\"cal cal-h2\">Byte Pair Encoding</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317b228",
   "metadata": {},
   "source": [
    "The Byte Pair Encoding is fairly simple to implement. We start by splitting each word into its characters, appending a special end-of-word symbol `</w>` to the end of each word. Then, we repeatedly find the most common adjacent pair of symbols across all words and merge them into a new symbol. This process is repeated for a specified number of merges.\n",
    "\n",
    "Once we have learned the merges, we can use them to tokenize new words by applying the merges in order until no more merges can be applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06487cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "EOW = \"</w>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_vocab(corpus: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create an initial vocabulary consisting of all single characters\n",
    "    observed in the corpus PLUS the EOW marker. IDs are assigned\n",
    "    deterministically (sorted).\n",
    "\n",
    "    Returns:\n",
    "        vocab: dict mapping token string -> integer id\n",
    "    \"\"\"\n",
    "    import string\n",
    "    ALL_ASCII = set(string.ascii_letters + string.digits + string.punctuation)\n",
    "    ALL_ASCII.add(EOW)  # include the end-of-word symbol\n",
    "    corpus_set = set(corpus) - set(' \\n\\t\\r')  # exclude whitespace\n",
    "\n",
    "    return {tok: i for i, tok in enumerate(corpus_set | ALL_ASCII)}\n",
    "\n",
    "vocab = build_initial_vocab(\"CS189 is CS.\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ebf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_char_seq_with_eow(corpus: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert the whole corpus into a single flat sequence of symbols.\n",
    "    Each word contributes its characters followed by the end-of-word marker `</w>`.\n",
    "    \n",
    "    Example:\n",
    "        corpus = \"low lower\"\n",
    "        returns: ['l','o','w','</w>','l','o','w','e','r','</w>']\n",
    "    \n",
    "    Why this matters:\n",
    "    - We treat the corpus as *one long list* (not a list of per-word lists),\n",
    "      which is sometimes more convenient for teaching and for demonstrating\n",
    "      the role of `</w>` in preventing merges across word boundaries.\n",
    "    \"\"\"\n",
    "    seq: List[str] = []\n",
    "    for word in corpus.split():\n",
    "        seq.extend(list(word))\n",
    "        seq.append(EOW)\n",
    "    return seq\n",
    "\n",
    "print(corpus_to_char_seq_with_eow(\"CS189 is great!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b658805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pair_frequencies(seq: List[str]) -> Counter:\n",
    "    \"\"\"\n",
    "    Count frequencies of adjacent symbol pairs over the *flat* sequence.\n",
    "    \n",
    "    Boundary rule:\n",
    "    - We *disallow* pairs that START with `</w>` because that would cross a \n",
    "      word boundary on merge (i.e., merging `</w>` with the next word's first\n",
    "      character). We DO allow pairs that END with `</w>` (e.g., ('w','</w>')),\n",
    "      which forms tokens like 'w</w>' and is standard in BPE.\n",
    "    \n",
    "    Returns:\n",
    "        A Counter mapping (left_symbol, right_symbol) -> count.\n",
    "    \"\"\"\n",
    "    pair_counts = Counter()\n",
    "    for i in range(len(seq) - 1):\n",
    "        left, right = seq[i], seq[i + 1]\n",
    "        if left.endswith(EOW): # This pair would cross a word boundary; skip it.\n",
    "            continue\n",
    "        pair_counts[(left, right)] += 1\n",
    "    return pair_counts\n",
    "\n",
    "corpus = \"CS189 is CS.\"\n",
    "seq = corpus_to_char_seq_with_eow(corpus)\n",
    "pair_freqs = count_pair_frequencies(seq)\n",
    "print(pair_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair_in_sequence(seq: List[str], pair: Tuple[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform a single merge of the given pair across the flat sequence.\n",
    "    Invariant:\n",
    "    - Never merge if the left symbol ends with `</w>` \n",
    "       (prevents crossing word boundaries).\n",
    "    - Scans left-to-right and uses a simple skip mechanic to avoid overlapping merges.\n",
    "    \"\"\"\n",
    "    a, b = pair\n",
    "    merged_token = a + b\n",
    "    new_seq: List[str] = []\n",
    "    i = 0\n",
    "    n = len(seq)\n",
    "    while i < n:\n",
    "        if i < n - 1 and seq[i] == a and seq[i + 1] == b and seq[i] != EOW:\n",
    "            new_seq.append(merged_token)\n",
    "            i += 2  # skip the merged pair\n",
    "        else:\n",
    "            new_seq.append(seq[i])\n",
    "            i += 1\n",
    "    return new_seq\n",
    "\n",
    "corpus = \"CS189 is CS.\"\n",
    "seq = corpus_to_char_seq_with_eow(corpus)\n",
    "pair_freqs = count_pair_frequencies(seq)\n",
    "pair, freq = pair_freqs.most_common(1)[0]\n",
    "print(\"Merging pair:\", pair, \"with frequency:\", freq)\n",
    "new_seq = merge_pair_in_sequence(seq, pair)\n",
    "print(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcea24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # pip install tqdm\n",
    "\n",
    "def learn_bpe_merges(corpus: str, num_merges: int = 1000, min_frequency: int = 2) -> Tuple[List[Tuple[str, str]], dict]:\n",
    "    \"\"\"\n",
    "    Learn BPE merge rules from the corpus by repeatedly finding the most frequent\n",
    "    adjacent pair and merging it, subject to the boundary rule.\n",
    "\n",
    "    Args:\n",
    "        corpus: Raw text (spaces separate words).\n",
    "        num_merges: Maximum number of merges to learn.\n",
    "        min_frequency: Stop when the most frequent pair occurs fewer than this.\n",
    "\n",
    "    Returns:\n",
    "        merges: A list of (left_symbol, right_symbol) in the order they were learned.\n",
    "        vocab: Final vocabulary mapping token -> id\n",
    "    \"\"\"\n",
    "    seq = corpus_to_char_seq_with_eow(corpus)\n",
    "    merges: List[Tuple[str, str]] = []\n",
    "    vocab = build_initial_vocab(corpus)\n",
    "    next_id = max(vocab.values()) + 1\n",
    "\n",
    "    # Wrap the merge loop in a tqdm progress bar\n",
    "    progress = tqdm(range(num_merges), desc=\"Learning BPE merges\", ncols=80)\n",
    "\n",
    "    for step in progress:\n",
    "        pair_counts = count_pair_frequencies(seq)\n",
    "        if not pair_counts:\n",
    "            progress.set_postfix_str(\"done (no pairs left)\")\n",
    "            break\n",
    "        (best_pair, best_count) = pair_counts.most_common(1)[0]\n",
    "        if best_count < min_frequency:\n",
    "            progress.set_postfix_str(f\"stopped (min freq < {min_frequency})\")\n",
    "            break\n",
    "\n",
    "        # Merge and update structures\n",
    "        seq = merge_pair_in_sequence(seq, best_pair)\n",
    "        merges.append(best_pair)\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        if new_token not in vocab:\n",
    "            vocab[new_token] = next_id\n",
    "            next_id += 1\n",
    "\n",
    "        # Update the tqdm progress bar info\n",
    "        progress.set_postfix_str(f\"merge {best_pair} ({best_count})\")\n",
    "\n",
    "    progress.close()\n",
    "    return merges, vocab\n",
    "\n",
    "corpus = \"This is the best CS class. This is CS 189.\"\n",
    "merges, vocab = learn_bpe_merges(corpus, num_merges=100, min_frequency=2)\n",
    "print(\"Learned merges:\", merges)\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode(text: str, merges: List[Tuple[str, str]], vocab: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Encode a string into token IDs:\n",
    "      1) Convert text -> flat char+EOW sequence\n",
    "      2) Apply learned merges in order\n",
    "      3) Map final tokens to IDs via vocab\n",
    "\n",
    "    Note:\n",
    "    - This simple teaching encoder applies merges globally; it assumes the\n",
    "      learned merges were derived from a similar distribution (your corpus).\n",
    "    - For speed, production systems use a 'rank' map and greedy longest-match;\n",
    "      here we stick to the clearest didactic approach.\n",
    "    \"\"\"\n",
    "    progress = tqdm(range(len(merges)), desc=\"Applying BPE merges\", ncols=80)\n",
    "    seq = corpus_to_char_seq_with_eow(text)\n",
    "    for a, b in merges:\n",
    "        seq = merge_pair_in_sequence(seq, (a, b))\n",
    "        progress.update(1)\n",
    "    progress.close()\n",
    "    return seq, [vocab[tok] for tok in seq]\n",
    "\n",
    "corpus = \"This is the best CS class. This is CS 189 the best class.\"\n",
    "merges, vocab = learn_bpe_merges(corpus, num_merges=100, min_frequency=2)\n",
    "encoded_seq, token_ids = bpe_encode(\"CS 189 is the   best   class.\", merges, vocab)\n",
    "print(\"Encoded sequence:\", encoded_seq)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_decode(token_ids: List[int], vocab: Dict[str, int]) -> str:\n",
    "    \"\"\"\n",
    "    Decode token IDs back to text by inverting the vocab and then\n",
    "    removing EOW markers to re-insert spaces.\n",
    "\n",
    "    Rules:\n",
    "    - Tokens that END with EOW represent end-of-word units.\n",
    "      We strip the trailing `</w>` and insert a space.\n",
    "    - Other tokens are just literal substrings inside a word.\n",
    "\n",
    "    Caveat:\n",
    "    - Because we concatenated strings to form merged tokens, decoding simply\n",
    "      concatenates their surfaces; then we rely on `</w>` to restore spaces.\n",
    "    \"\"\"\n",
    "    inv_vocab = {i: t.replace(EOW, \" \") for t, i in vocab.items()}\n",
    "    out_words: List[str] = []\n",
    "    buf = [inv_vocab[tid] for tid in token_ids]\n",
    "    return \"\".join(buf).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a929d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = bpe_decode(token_ids, vocab)\n",
    "print(f\"Decoded text: \\\"{decoded_text}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bd762",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2  class=\"cal cal-h2\">Implementing the Decoder Transformer for Generative Pre-training</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ee11b",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3  class=\"cal cal-h3\">Getting the Data</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"shakespeare.txt\"):\n",
    "    print('downloading corpus...')\n",
    "    import requests\n",
    "    url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    "    response = requests.get(url)\n",
    "    shakespeare_corpus = response.text\n",
    "    with open(\"shakespeare.txt\", \"w\") as f:\n",
    "        f.write(shakespeare_corpus)\n",
    "else:\n",
    "    print('loading cached file...')\n",
    "    with open(\"shakespeare.txt\", \"r\") as f:\n",
    "        shakespeare_corpus = f.read()\n",
    "print(f\"Corpus length: {len(shakespeare_corpus)} characters\") \n",
    "print(shakespeare_corpus[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e50d5b",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3  class=\"cal cal-h3\">Byte Pair Encoding</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33181dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"bpe_state.pkl\"):\n",
    "#     print('learning BPE merges on Shakespeare corpus...')\n",
    "#     merges, vocab = learn_bpe_merges(shakespeare_corpus, \n",
    "#                                      num_merges=200, \n",
    "#                                      min_frequency=2)\n",
    "#     with open(\"bpe_state.pkl\", \"wb\") as f:\n",
    "#         pickle.dump((merges, vocab), f)\n",
    "# else:\n",
    "#     print(\"loading cached BPE state...\")\n",
    "#     with open(\"bpe_state.pkl\", \"rb\") as f:\n",
    "#         merges, vocab = pickle.load(f)\n",
    "# print(\"Learned merges:\", merges)\n",
    "# print(\"Vocabulary:\", vocab)\n",
    "# vocab_size = len(vocab)\n",
    "# print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"encoded_text_ids.pkl\"):\n",
    "#     print('encoding Shakespeare corpus...')\n",
    "#     encoded_seq, token_ids = encode(shakespeare_corpus, merges, vocab)\n",
    "#     with open(\"encoded_text_ids.pkl\", \"wb\") as f:\n",
    "#         pickle.dump((encoded_seq,token_ids), f)\n",
    "# else:\n",
    "#     print(\"loading cached encoded text...\")\n",
    "#     with open(\"encoded_text_ids.pkl\", \"rb\") as f:\n",
    "#         encoded_seq, token_ids = pickle.load(f)\n",
    "# print(\"Encoded sequence length:\", len(encoded_seq))\n",
    "# corpus_tokens = torch.tensor(token_ids, dtype=torch.long, device=device)\n",
    "\n",
    "# def encode(text: str) -> torch.Tensor:\n",
    "#     _, token_ids = bpe_encode(text, merges, vocab)\n",
    "#     return torch.tensor(token_ids, dtype=torch.long, device=device)\n",
    "\n",
    "# def decode(token_ids: torch.Tensor) -> str:\n",
    "#     return bpe_decode(token_ids.tolist(), vocab)\n",
    "\n",
    "# tok = encode(\"To be, or not to be, that is the question.\")\n",
    "# decode(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6676c2",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3  class=\"cal cal-h3\">Word Encoding</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def split_words(corpus: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Break the corpus into words using a regex that matches word characters.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b\\w+\\b'\n",
    "    corpus = re.sub(r'[._,!?;\"\\'`()\\[\\]{}<>]', '', corpus.lower())\n",
    "    return re.findall(pattern, corpus.lower())\n",
    "\n",
    "words = split_words(shakespeare_corpus)\n",
    "# counter = Counter(words)\n",
    "# vocab_set = {tok for tok, cnt in counter.items() if cnt > 1}\n",
    "vocab_set = set(words)\n",
    "vocab = {word: i for i, word in enumerate(sorted(vocab_set), start = 1)}\n",
    "vocab[\"<unknown>\"] = 1\n",
    "inv_vocab = {i: word for word, i in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "\n",
    "def encode(text: str):\n",
    "    \"\"\"\n",
    "    Encode a string into token IDs using the provided vocabulary.\n",
    "    Unknown words are mapped to the ID for <unknown>.\n",
    "    \"\"\"\n",
    "    words = split_words(text)\n",
    "    return torch.tensor(\n",
    "        [vocab.get(word, 1) for word in words],\n",
    "        dtype=torch.long, device=device)\n",
    "\n",
    "def decode(tokens: torch.Tensor) -> str:\n",
    "    \"\"\"\n",
    "    Decode token IDs back to text by inverting the vocabulary.\n",
    "    \"\"\"\n",
    "    words = [inv_vocab.get(t.item(), \"<error>\") for t in tokens]\n",
    "    return \" \".join(words)\n",
    "\n",
    "corpus_tokens = encode(shakespeare_corpus)\n",
    "print(\"Length of token IDs:\", len(corpus_tokens))\n",
    "decode(encode(\"to be or not to be that is the question tokenizer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1d2c3",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3  class=\"cal cal-h3\">Data Preparation</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcccdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = corpus_tokens.shape[0]\n",
    "seq_length = 64\n",
    "split_ratio = 0.90\n",
    "seed = 189\n",
    "x = corpus_tokens[:(N - (N % seq_length) + 1)]\n",
    "y = x[1:].reshape(-1, seq_length)\n",
    "x = x[:-1].reshape(-1, seq_length)\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "dataset = TensorDataset(x, y)\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "training_data, validation_data = random_split(\n",
    "    dataset, [split_ratio, 1 - split_ratio], \n",
    "    generator=generator) \n",
    "print(\"training contexts\", len(training_data))\n",
    "print(\"validation contexts\", len(validation_data))\n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(training_data[0][0]))\n",
    "print(decode(training_data[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fc382",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3  class=\"cal cal-h3\">Attention</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: torch.Tensor, \n",
    "                                 K: torch.Tensor, \n",
    "                                 V: torch.Tensor, \n",
    "                                 mask=None):\n",
    "    \"\"\"\n",
    "    Q: matrix of shape (B, N, d_k)\n",
    "    K: matrix of shape (B, N, d_k)\n",
    "    V: matrix of shape (B, N, d_v)\n",
    "    mask: boolean matrix of shape (1, N, N). Values where mask is True will be INCLUDED\n",
    "    \"\"\"\n",
    "    (B, N, d_k) = Q.shape\n",
    "    (_, _, d_v) = V.shape\n",
    "    K_T = K.transpose(-2, -1) # (B, d_k, N)\n",
    "    dot_product = (Q @ K_T) / (d_k ** 0.5) # (B, N, N)\n",
    "    \n",
    "    if mask is not None:\n",
    "        dot_product = dot_product.masked_fill(mask.logical_not(), float('-inf'))\n",
    "\n",
    "    attention = F.softmax(dot_product, dim=-1) # (B, N, N)\n",
    "    return attention @ V  # (B, N, N) * (B, N, d_v) = (B, N, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mask_cache = {}\n",
    "def get_mask_with_cache(N, device):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular mask of shape (1, N, N) to be used for masked attention.\n",
    "    \"\"\"\n",
    "    if N not in _mask_cache:\n",
    "        _mask_cache[N] = torch.ones(\n",
    "            (N, N), dtype=torch.bool, \n",
    "            device=device).tril().unsqueeze(0)  \n",
    "    return _mask_cache[N] #  (1, N, N)\n",
    "\n",
    "class MaskedAttentionHead(nn.Module):\n",
    "    def __init__(self, d_model=512, d_v=512, d_k=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_k, bias = False)\n",
    "        self.W_q = nn.Linear(self.d_model, self.d_k, bias = False)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_v, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is the input to use for the queries, keys, and values\n",
    "        encoder_output is the output from the encoder (used for cross-attention)\n",
    "        mask is the mask to use for the attention\n",
    "        \"\"\"\n",
    "        (B, N, _) = x.shape\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        mask = get_mask_with_cache(N, device=x.device)\n",
    "        values = scaled_dot_product_attention(Q, K, V, mask=mask) \n",
    "        # ##  more efficient implementation:\n",
    "        # ##  Need to unsqueeze the head dimension for F.scaled_dot_product_attention\n",
    "        # values = F.scaled_dot_product_attention(\n",
    "        #     query=Q, key=K, value=V, \n",
    "        #     attn_mask=mask)\n",
    "        return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb5417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=8, d_model=512, d_k=64):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.attention_heads = nn.ModuleList(\n",
    "            [\n",
    "                MaskedAttentionHead(d_model=self.d_model, d_v = self.d_v, d_k=self.d_k)\n",
    "                for _ in range(self.num_heads)\n",
    "            ]\n",
    "        )\n",
    "        # Projection\n",
    "        self.W_out = nn.Linear(self.num_heads * self.d_v, self.d_model) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        (B, N, _) = x.shape\n",
    "        assert(x.shape == (B, N, self.d_model))\n",
    "        head_outputs = [head(x) for head in self.attention_heads]\n",
    "        for head_out in head_outputs:\n",
    "            assert(head_out.shape == (B, N, self.d_v))\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        assert(concatenated.shape == (B, N, self.num_heads * self.d_v))\n",
    "        out = self.W_out(concatenated)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6bd0f0",
   "metadata": {},
   "source": [
    "Testing the Layer with Masked Multi-Head Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb232e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(vocab_size, 512).to(device)\n",
    "layer = MaskedMultiHeadAttention(7, 512, 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "x, y = training_data[:batch_size]\n",
    "emb(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64117bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(emb(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af3c92",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3  class=\"cal cal-h3\">Decoder Architecture</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, d_k=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ffn = 4 * self.d_model\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mh_attention = MaskedMultiHeadAttention(\n",
    "            num_heads=self.num_heads, \n",
    "            d_model=self.d_model,\n",
    "            d_k=self.d_k)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_ffn),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(self.d_ffn, self.d_model),\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(self.d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(self.d_model)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mh = self.mh_attention(self.layernorm1(x)) # Prenorm\n",
    "        mh = self.dropout(mh)\n",
    "        x = x + mh\n",
    "        ffn = self.ffn(self.layernorm2(x)) # Prenorm\n",
    "        ffn = self.dropout(ffn)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = DecoderBlock(d_model=512, num_heads=8, d_k=64, dropout=0.1).to(device)\n",
    "block(emb(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, max_len: int = 1024, L: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Sinusoidal positional encoding as in 'Attention is All You Need'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        pos = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
    "        positions = torch.arange(max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_terms = L ** (torch.arange(0, d_model, 2, dtype=torch.float32) / d_model)\n",
    "        quotient = positions / div_terms\n",
    "        pos[:, 0::2] = torch.sin(quotient)  # even indices\n",
    "        pos[:, 1::2] = torch.cos(quotient)  # odd indices\n",
    "        # Register as non-parameter buffer so it moves with the module\n",
    "        self.register_buffer(\"pos\", pos)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        # pos: (1, seq_len, d_model) broadcasts along batch dimension\n",
    "        return x + self.pos[:seq_len].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderOnly(nn.Module):\n",
    "    def __init__(self, \n",
    "                 max_length=1024,\n",
    "                 vocab_size=6000, \n",
    "                 d_model=512,\n",
    "                 d_k=64,\n",
    "                 num_layers=6, \n",
    "                 num_heads=8, \n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_k = d_k\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.layers = nn.Sequential()\n",
    "        self.layers.append(\n",
    "            nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        )\n",
    "        self.layers.append(\n",
    "            PositionalEncoding(d_model=d_model, max_len=max_length, L=10000)\n",
    "        )\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(\n",
    "                DecoderBlock(d_model=d_model, num_heads=num_heads, \n",
    "                             d_k=self.d_k, dropout=self.dropout)\n",
    "            )\n",
    "        self.layers.append(\n",
    "            nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "        )\n",
    "        \n",
    "    def num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        \"\"\"\n",
    "        x: (B, N) tensor of input token IDs\n",
    "        max_new_tokens: number of tokens to generate\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            B, N = x.shape\n",
    "            for _ in range(max_new_tokens):\n",
    "                x = x[:, -seq_length:]  # crop to last seq_length tokens\n",
    "                logits = self.forward(x)  # (B, N, vocab_size)\n",
    "                next_token_logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "                next_token_probs = F.softmax(next_token_logits, dim=-1)  # (B, vocab_size)\n",
    "                next_tokens = torch.multinomial(next_token_probs, num_samples=1)  # (B, 1)\n",
    "                x = torch.cat([x, next_tokens], dim=1)  # (B, N+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerDecoderOnly(\n",
    "    max_length=seq_length, \n",
    "    vocab_size=vocab_size, \n",
    "    d_model=256, d_k=16, num_layers=6, num_heads=8, \n",
    "    dropout=0.1).to(device)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of parameters:\", model.num_parameters()/1e6, \"million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(model.generate(encode(\"to be or not to be\").unsqueeze(0), max_new_tokens=20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fe408",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3  class=\"cal cal-h3\">Training Loop</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cross_entropy(pred, y):\n",
    "    # flatten the batch into a single dimension and \n",
    "    # compute cross-entropy\n",
    "    return F.cross_entropy(pred.view(-1, vocab_size), y.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b364f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_cross_entropy(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00683cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gd(model, loss_fn, \n",
    "                 training_data,\n",
    "                 batch_size, \n",
    "                 nsteps, \n",
    "                 learning_rate,\n",
    "                 visualizer=None,\n",
    "                 weight_decay=1e-4):\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(189)\n",
    "    loader = DataLoader(training_data, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, # shuffles each epoch\n",
    "                        generator=generator)\n",
    "    \n",
    "    # Define the optimizer (this is the update rule)\n",
    "    # Alternatively, you can use Adam optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "    model.train() # set model to training mode (important for dropout/batchnorm)\n",
    "    step = 0\n",
    "    # Loop through the steps\n",
    "    iter_loader = iter(loader)\n",
    "    for step in tqdm(range(nsteps)):\n",
    "        # Get the next batch of data\n",
    "        try:\n",
    "            x, t = next(iter_loader)\n",
    "        except StopIteration:\n",
    "            iter_loader = iter(loader)\n",
    "            x, t = next(iter_loader)\n",
    "        # Zero the gradients to start the next step\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, t)\n",
    "        tr_loss = loss.item()\n",
    "        # Backpropagation (compute the gradient)\n",
    "        loss.backward()\n",
    "        # Update the parameters using the optimizer's update rule\n",
    "        optimizer.step()\n",
    "        # Visualize the model (if a visualizer function is provided)\n",
    "        if visualizer is not None:\n",
    "            model.eval() # disable dropout/batchnorm\n",
    "            with torch.no_grad():\n",
    "                visualizer(step, model, loss_fn, tr_loss)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33651f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossVisualizer:\n",
    "    def __init__(self, loss_fig, validation_data):\n",
    "        self.loss_fig = loss_fig\n",
    "        self.val_loader = DataLoader(validation_data, \n",
    "                                     batch_size=32, \n",
    "                                     shuffle=False)\n",
    "        self.epochs = []\n",
    "        self.losses_val = []\n",
    "        self.losses_tr = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.epochs = []\n",
    "        self.losses_val = []\n",
    "        self.losses_tr = []\n",
    "        with self.loss_fig.batch_update():\n",
    "            self.loss_fig.data[0].x = []\n",
    "            self.loss_fig.data[0].y = []\n",
    "            self.loss_fig.data[1].x = []\n",
    "            self.loss_fig.data[1].y = []\n",
    "    \n",
    "    def __call__(self, epoch, model, loss_fn, loss_tr):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            for x_val, t_val in self.val_loader:\n",
    "                loss_val = loss_fn(model(x_val), t_val).item()\n",
    "                losses.append(loss_val)\n",
    "            loss_val = np.mean(losses)\n",
    "        self.epochs.append(epoch)\n",
    "        self.losses_val.append(loss_val)\n",
    "        self.losses_tr.append(loss_tr)\n",
    "        print(\"training loss:\", loss_tr, \"validation loss:\", loss_val)\n",
    "        # Visualization Code\n",
    "        with self.loss_fig.batch_update():\n",
    "            self.loss_fig.data[0].x = self.epochs\n",
    "            self.loss_fig.data[0].y = self.losses_val\n",
    "            self.loss_fig.data[1].x = self.epochs\n",
    "            self.loss_fig.data[1].y = self.losses_tr\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fig = go.FigureWidget()\n",
    "loss_fig.add_trace(go.Scatter(x=[0], y=[0], mode='lines', name='Val. Loss'))\n",
    "loss_fig.add_trace(go.Scatter(x=[0], y=[0], mode='lines', name='Train. Loss'))\n",
    "visualizer = LossVisualizer(loss_fig, validation_data)\n",
    "display(loss_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ba679",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.reset()\n",
    "model = TransformerDecoderOnly(\n",
    "    max_length=seq_length, \n",
    "    vocab_size=vocab_size, \n",
    "    d_model=1024, d_k=32, num_layers=2, num_heads=8, \n",
    "    dropout=0.0).to(device)\n",
    "\n",
    "#model = torch.compile(model)\n",
    "\n",
    "# model = TransformerDecoderOnly(\n",
    "#     max_length=seq_length, \n",
    "#     vocab_size=vocab_size, \n",
    "#     d_model=128, d_k=32, num_layers=8, num_heads=8, \n",
    "#     dropout=0.1).to(device)\n",
    "\n",
    "minibatch_gd(\n",
    "    model=model,\n",
    "    loss_fn=batch_cross_entropy,\n",
    "    training_data=training_data,\n",
    "    batch_size=128,\n",
    "    nsteps=200,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-7,\n",
    "    visualizer=visualizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af5a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(model.generate(encode(\"whether tis nobler\").unsqueeze(0), max_new_tokens=20)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b39d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
