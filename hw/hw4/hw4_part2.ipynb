{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT: On Colab, we expect your homework to be in the cs189 folder\n",
    "## Please contact staff if you encounter any problems with installing dependencies\n",
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/cs189/hw/hw4\n",
    "    %pip install -r ./requirements.txt\n",
    "    !pip install -U kaleido plotly\n",
    "    import kaleido\n",
    "    kaleido.get_chrome_sync()\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = pio.renderers.default + \"+png\"\n",
    "\n",
    "# Last updated: 2025-11-07 01:26:19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw4_part2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieyo2cn9gOER"
   },
   "source": [
    "# Homework 4.2\n",
    "\n",
    "The limit of what's possible is constantly being redefined in the world of AI and ML. Researcher, engineers, and curious scholars just like me and you have invented creative ways to apply machine learning everywhere! In this homework, we're going to explore how the ideas behind transformers and convolutional neural networks can solve problems in unconventional ways---and hopefully inspire you to experiment in out-of-the-box ways on your own too.\n",
    "\n",
    "Vision models (CNNs, ResNets) take in images as input, where the images are represented by multi-dimensional arrays of pixel values. Transformers take in *tokens* as input, where each chunk of the data is encoded into a number. Vision Transformers apply this same idea to images by encoding *patches* of images into tokens.\n",
    "\n",
    "A key idea we hope you take away from this homework is how incredibly versatile machine learning can be. Machine learning models love to work with numbers---matrices, vectors, and tensors---so as long as we can represent our data as a number in some way, we can hope that our models can learn something meaningful from them!\n",
    "\n",
    "---\n",
    "\n",
    "## Due Date: Friday, November 28, 11:59 PM\n",
    "\n",
    "This assignment is due on **Friday, November 28, at 11:59 PM**. You must submit your work to Gradescope by this deadline. Please refer to the syllabus for the [Slip Day policy](https://eecs189.org/fa25/syllabus/#slip-days). No late submissions will be accepted beyond the details outlined in the Slip Day policy.\n",
    "\n",
    "\n",
    "**Start early! You only have a limited number of Kaggle submissions per day.**\n",
    "\n",
    "## Deliverables:\n",
    "* Homework 4.2 consists of coding questions and **2 Kaggle competition submissions**.\n",
    "* Code [PDF]:\n",
    "    * Please submit a **PDF** of your coding notebook to the `HW 4.2 Coding [PDF]` assignment on Gradescope.\n",
    "    * Please tag the pages in your PDF to the corresponding questions on Gradescope.\n",
    "    * In addition to the code from your notebook, make sure you include a screenshot of your **scores to the 2 Kaggle competitions** and your **Kaggle username**.\n",
    "* Code [Notebook]:\n",
    "    * Please submit the zip file generated by the final export cell in this coding notebook to the `HW 4.2 Coding [Code]` assignment on Gradescope.\n",
    "    * Code submitted in your notebook must match the code in your PDF submission, and produce the *exact* same outputs if re-run by course staff. Inconsistent or incomplete code might not receive marks.\n",
    "* Kaggle Competition Submissions:\n",
    "    * Submit your predictions for 2 Kaggle competitions.\n",
    "        * [Fa25 CS189 HW 4 - DNABERT](https://www.kaggle.com/competitions/fa-25-cs-189-hw-4-competition/overview)\n",
    "        * [Fa25 CS189 HW4 - UrbanSound8k](https://www.kaggle.com/competitions/fa-25-cs-189-hw-4-urban-sounds-8-k/overview)\n",
    "    * Include your **Kaggle username** and a screenshot of your **Kaggle scores** in your PDF submission.\n",
    "* Note: Make sure you submit *both* a PDF version of your notebook (to the Coding [PDF] assignment) and a zip file of the notebook (to the Coding [Code] assignment) on Gradescope.\n",
    "\n",
    "### Submission Tips:\n",
    "- **Plan ahead:** We strongly encourage you to submit your work several hours before the deadline. This will give you ample time to address any submission issues.\n",
    "- **Reach out for help early:** If you encounter difficulties, contact course staff well before the deadline. While we are happy to assist with submission issues, we cannot guarantee responses to last-minute requests.\n",
    "- **Checkpoints:** The two subparts are independent of each other. If you only want to work on subpart 2 in a separate sitting, all you need to do is run the otter setup cell, Colab initialization cell, import cell, and seed setting cell before going directly to subpart 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "This notebook contains a series of tasks designed to help you practice and apply key concepts in data manipulation and visualization. You will complete all the TODOs in the notebook, which include both coding and written response questions. Some tasks are open-ended, which allows you to explore and experiment with different approaches.\n",
    "\n",
    "### Key Learning Objectives:\n",
    "1. Gain familiarity with bi-directional encoders and ConvNext architectures\n",
    "2. Implement custom tokenizers and datasets for data of different modalities (text, images, and sound)\n",
    "3. Practice writing your own training loops and analyzing training performance\n",
    "4. Learn how to add classification layers to different architectures\n",
    "5. Learn how to load, freeze, and finetune pre-trained models\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "### Grading Breakdown\n",
    "| Question | Points |\n",
    "|----------|--------|\n",
    "| 4a       | 2      |\n",
    "| 4b       | 1      |\n",
    "| 4c       | 2      |\n",
    "| 4d       | 2      |\n",
    "| 4e       | 2      |\n",
    "| 4f       | 3      |\n",
    "| 4g       | 4      |\n",
    "| 4h       | 1      |\n",
    "| 4i (Kaggle) | 3      |\n",
    "| 5a       | 3      |\n",
    "| 5b       | 2      |\n",
    "| 5c       | 3      |\n",
    "| 5d       | 1      |\n",
    "| 5e       | 3      |\n",
    "| 5f       | 3      |\n",
    "| 5g       | 3      |\n",
    "| 5h       | 2      |\n",
    "| 5i       | 2      |\n",
    "| 5j (Kaggle) | 3      |\n",
    "| **Total**| **45** |\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "1. Carefully read each question and its requirements.\n",
    "2. Complete all TODOs in the notebook. You may add extra lines of code if needed to implement your solution.\n",
    "3. For manual questions, provide clear and concise written responses.\n",
    "4. Test your code thoroughly to ensure it meets the requirements.\n",
    "\n",
    "Good luck! Autobots, roll out! ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNlPQCDvgQ3W"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objects\n",
    "import plotly.subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNbn7nMKgSle"
   },
   "source": [
    "### **IMPORTANT:**\n",
    "- Do not change the random seed values!!!\n",
    "- Before you submit your notebook, remember to set `save_models=True` and `load_models=True`. This saves your final models which we will use for the autograder. Set these to false if you are still tweaking your model setup. We have provided code for saving models - **do not change these file names!!**\n",
    "- When uploading your notebook, make sure to include your model file `classifier.joblib` in your submission\n",
    "\n",
    "**Reminder:**\n",
    "- Subpart 1 and subpart 2 are independent of each other! If you want to work directly on subpart 2, you don't have to rerun all the cells in subpart 1. You just need to run everything before this cell, and also the cell below before skipping directly to subpart 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "kR5jhkRugRNz",
    "outputId": "ebb26406-d35a-4213-e92c-cf9d340bcbba"
   },
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    \"\"\"\n",
    "    Set the seed for all random number generators\n",
    "    \"\"\"\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Set random seeds for reproducible results\n",
    "SEED = 42\n",
    "set_seed()\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Use hardware accelerator if available\n",
    "    \"\"\"\n",
    "    if torch.accelerator.is_available():\n",
    "        device = torch.accelerator.current_accelerator()\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "IS_GRADING_ENV = os.getenv(\"IS_GRADING_ENV\") == \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZLlhL9TgXix"
   },
   "source": [
    "# Subpart 1: Tokens are All You Need - DNABERT\n",
    "\n",
    "## Recap: The Full Transformer Architecture\n",
    "\n",
    "In part 1 of this homework, we built our own transformer end-to-end. The original transformer model architecture consists of two main parts: an encoder and a decoder.\n",
    "\n",
    "* **Encoder:** The encoder processes the input sequence (for example, a sentence). Each encoder layer applies self-attention, allowing every position in the input to attend to every other position, plus a feed-forward network. The output is a collection of contextualized embeddings, one for each token.\n",
    "* **Decoder:** The decoder is designed to generate output sequences step by step (e.g., translating a sentence or generating text). At each step, the decoder takes in all previously generated outputs and the encoder's outputs, using self-attention (with causal masking so tokens can only \"see\" earlier positions) and cross-attention layers (which let the decoder attend to the encoder's output). This setup is especially important for tasks like machine translation, where the model learns to generate output one token at a time.\n",
    "\n",
    "The full encoder-decoder architecture is useful in applications where end-to-end translation or generation from one domain to another is needed. For instance, sequence-to-sequence tasks that involve processing a source sentence and generating a target sequence based on the source sentence.\n",
    "\n",
    "Most modern transformer variants (like BERT, DNA BERT, and GPT) drop either the decoder or encoder to specialize in different types of tasks:\n",
    "* Encoder-only: Sequence understanding (e.g., BERT, DNA BERT)\n",
    "* Decoder-only: Sequence generation (e.g., GPT)\n",
    "\n",
    "## BERT\n",
    "The Bi-Directional Encoder Representations from Transformers is a language model invented by Google researchers in October 2018.\n",
    "\n",
    "* Encoder-only architectures like BERT and BERT-based models (like DNA BERT) use only the encoder part of the transformer, not the decoder. The encoder stack produces contextualized embeddings for each token in the input sequence.\n",
    "* This encoder-only design is standard for models focused on sequence classification or embedding tasks, as opposed to sequence generation.\n",
    "\n",
    "### What is a \"bi-directional encoder\"?\n",
    "A bi-directional encoder in BERT means that the model processes the entire input sequence at once, considering both the left and right context for every token. Traditional language models process sequences left-to-right: when predicting the token at position $t$, the transformer's decoder only looks tat tokens from position 1 to $t-1$ (this is why we needed to use masking for decoder self-attention!). Unlike traditional language models that read text only left-to-right, BERT's encoder uses self-attention to look at all tokens simultaneously, allowing it to learn richer, context-aware representations. As a result, each token's representation is influenced by all other tokens in the sequence, both before and after it. This is crucial for the model to understand the full context of DNA or text sequences.\n",
    "\n",
    "## Tokenizers\n",
    "A **tokenizer** is a tool or algorithm used in natural language processing (NLP) and deep learning that splits raw text (or sequence data) into smaller pieces called **tokens**. These tokens are the basic units that a model uses as input.\n",
    "\n",
    "* Purpose: Converts raw text (or DNA sequence, etc.) into tokens that can be mapped to numerical IDs and understood by models.\n",
    "* Tokens: Can be words, subwords, characters, etc.\n",
    "* Vocabulary: Each tokenizer has a vocabulary that maps each unique token to a numerical index (token ID).\n",
    "\n",
    "### How do tokenizers work?\n",
    "1. The text is split into tokens using specific rules (e.g., by spaces, punctuation, or custom logic).\n",
    "2. Each token is mapped to its corresponding ID in the vocabulary.\n",
    "3. Tokenizers can add special tokens like `[CLS]` (classification token), `[SEP]` (separator token), `[PAD]` (padding token), `[MASK]` (masking token), or `[UNK]` (token for unknown inputs) as required by the model architecture and task.\n",
    "\n",
    "#### Example (BERT for English):\n",
    "* Sentence: `\"I love dinosaurs\"`\n",
    "* Tokens: `[\"[CLS]\", \"I\", \"love\", \"dinosaurs\", \"[SEP]\"]`\n",
    "* Token IDs: `[101, 146, 1567, 4083, 102]` (example values)\n",
    "\n",
    "\n",
    "### Why do tokenizers matter?\n",
    "The pretrained model expects input in the form of token IDs corresponding to its vocabulary. Mismatched tokenization can lead to unknown tokens and poor performance.\n",
    "\n",
    "When working with pretrained transformer models like DNA BERT, you should also load the **pretrained tokenizer** that matches the model. The tokenizer is not just a text splitter---it has been trained by the researchers to convert input sequences into the correct token IDs based on the vocabulary the model expects. If you use a different tokenizer, the pretrained model may not perform as expected.\n",
    "\n",
    "For example, you'll often see code tutorials from PyTorch, HuggingFace, or Kaggle load in both the pretrained model and the tokenizer:\n",
    "```\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"<name_of_awesome_bert_model>\")\n",
    "model = AutoModel.from_pretrained(\"<name_of_awesome_bert_model>\")\n",
    "```\n",
    "\n",
    "## DNABERT\n",
    "<div align=\"center\">\n",
    "<img src=\"https://i.imgur.com/qJegHdR.jpeg\" alt=\"DNABERT Architecture\" width=\"400\"/>\n",
    "<br>\n",
    "<caption>\n",
    "<a href=\"https://academic.oup.com/bioinformatics/article/37/15/2112/6128680\">\n",
    "Source: Bioinformatics - Ji, Zhou, Liu, and Davuluri\n",
    "</a>\n",
    "</caption>\n",
    "</div>\n",
    "\n",
    "### What is DNABERT?\n",
    "DNABERT is a deep learning model that adapts the **BERT (Bidirectional Encoder Representations from Transformers)** architecture for DNA sequence analysis. DNA BERT takes in k-mers (DNA sequences split into groups of $k$ letters each) and outputs an embedding for each k-mer token.\n",
    "\n",
    "### The DNABERT-6 Architecture\n",
    "DNABERT-6 uses the same architecture as the original BERT model:\n",
    "* 12 transformer encoder layers\n",
    "* 768 hidden units per layer\n",
    "* 12 attention heads\n",
    "* ~110 million parameters\n",
    "\n",
    "DNABERT-6 adapts this architecture for DNA sequence data.\n",
    "\n",
    "### How is DNABERT-6 trained?\n",
    "It is pre-trained on large amounts of unlabeled genetic data using k-mer tokenization, enabling it to capture both local and long-range dependencies in DNA.\n",
    "\n",
    "The model is designed to be fine-tuned for various downstream tasks, such as promoter, splice site, or transcription factor binding site prediction, often with limited labeled data.\n",
    "\n",
    "###  How does DNABERT-6 tokenize DNA sequences?\n",
    "DNABERT-6 uses k-mer tokenization: each DNA sequence is split into overlapping substrings of length k (e.g., 6-mers for DNA_bert_6). For example, the sequence `ATGGCT` with k=3 becomes `ATG TGG GGC GCT`.\n",
    "\n",
    "The DNABERT-6 tokenizer expects the input as a space-separated string of k-mers. The DNABERT-6 tokenizer may also add special tokens at the start and end of a sequence of k-mers: `[CLS]` at the start (for classification) and `[SEP]` at the end.\n",
    "\n",
    "#### Example (DNABERT-6) with 3-mers:\n",
    "* Split the sequence `ATGGCT` into 3-mers: `\"ATG TGG GGC GCT\"`\n",
    "* Tokens: `[CLS], ATG, TGG, GGC, GCT, [SEP]`\n",
    "* Token IDs: `[101][1234][2345][3456][4567][102]` (example values)\n",
    "\n",
    "### What is the [CLS] token embedding and how is it used?\n",
    "The `[CLS]` token is a special token prepended to every input sequence by the DNA BERT Tokenizer. The `[CLS]` token's final hidden state (the encoder's output vector at position 0) is designed to represent the entire sequence.\n",
    "\n",
    "For classification tasks, the `[CLS]` embedding is typically fed into a classification layer (e.g., a linear layer) to predict the class label. This approach is explicitly described in the DNA BERT paper.\n",
    "\n",
    ">\"As illustrated in Figure 1b, for a DNA sequence, we tokenized it into a sequence of k-mers and added a special token `[CLS]` at the beginning of it (which represents the whole sequence) as well as a special token `[SEP]` at the end (which denotes the end of sequence).\"\n",
    "\n",
    ">Figure 1b caption: \"DNABERT uses tokenized k-mer sequences as input, which also contains a CLS token (a tag representing meaning of entire sentence), a SEP token (sentence separator) and MASK tokens (to represent masked k-mers in pre-training). The input passes an embedding layer and is fed to 12 Transformer blocks. The **first output among last hidden states will be used for sentence-level classification** while outputs for individual masked token used for token-level classification.\"\n",
    "\n",
    "In code, if your hidden state tensor is `[batch_size, sequence_length, hidden_size]`, then `hidden_state[:, 0, :]` gives you the `[CLS]` embedding for each sequence in the batch.\n",
    "\n",
    "\n",
    "### How do we use BERT-based models for classification?\n",
    "Since the pretrained BERT models usually don't include a classification head by default, we can add our own layer (e.g., a linear or MLP layer) at the end of BERT-based model to make predictions using the `[CLS]` token's embedding.\n",
    "\n",
    "The typical workflow is:\n",
    "1. Tokenize and encode the sequence into a list of integer Token IDs.\n",
    "2. Pass through the model to get the hidden states.\n",
    "3. Extract the `[CLS]` embedding (`hidden_state[:, 0, :]`).\n",
    "4. Feed this embedding into your classification layer and train on your labeled data.\n",
    "\n",
    "This is the standard approach for BERT-based models in both genomics and NLP.\n",
    "\n",
    "### What does the hidden state output of DNABERT look like?\n",
    "The model's output is a tensor of shape `[batch_size, sequence_length, hidden_size]` (e.g., `[10, 512, 768]`).\n",
    "\n",
    "The first dimension is the batch size, the second is the number of tokens passed as the input sequence to the DNABERT model (i.e. number of k-mers, up to the model's max length), and the third is the embedding size.\n",
    "\n",
    "To classify the sequence located at `batch_idx` in our batch, we use the vectors at position `[batch_idx, 0, :]` (the `[CLS]` token embedding) as the input to the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws0JcpkqjK40"
   },
   "source": [
    "## Question 4a: Load in DNA Data\n",
    "We have three text files: `chimpanzee_train.txt`, `dog_train.txt`, and `human_train.txt`. Each file contains DNA sequences for a different species.\n",
    "\n",
    "There are 2 columns in each text file:\n",
    "* `sequence`: the DNA string (A, T, C, G).\n",
    "* `class`: an integer that describes which gene family this DNA sequence comes from (G protein coupled receptors, receptors, Tyrosine kinase, Tyorsine phosphatase, Synthetase, Synthase, Ion channel, or Transcription factor). In this homework assignment, we won't actually be using the gene family as our `class`. Rather, we want to predict what species each DNA sequence comes from, so we will create our own `target` class based on whether this DNA sequence belongs to a chimpanzee, dog, or human.\n",
    "\n",
    "Let's load them in as Pandas Dataframes. We'll create a `target` column based on the species the DNA comes from to use for prediction. Ideally, when training our classifier, we want our model to see an equal proportion of all the classes. The number of sequences (rows) in each txt file is not the same, so we need to create a combined dataset with an even class distribution (i.e. same number of examples for each species). We can calculate which species has the fewest DNA sequences, sample exactly that number of DNA sequences from all three species, and concatenate them together into a combined dataframe to use for training later!\n",
    "\n",
    "Acknowledgements: ≈û√ºkr√º Ozan, [\"DNA Sequence Classification with Compressor\"](https://arxiv.org/abs/2401.14025), GitHub Repository: [Source](https://github.com/sukruozan/DNA-Sequence-Classification)\n",
    "\n",
    "**Task:**\n",
    "1. Load `chimpanzee_train.txt`, `dog_train.txt`, and `human_train.txt` as 3 dataframes.\n",
    "2. For each dataframe:\n",
    "    1. Create a column called `species` that contains a string representing what species (\"chimpanzee\", \"dog\", or \"human\") the DNA sequences comes from.\n",
    "    2. Create a column called `target` that contains an integer representing what species the DNA sequence comes from.\n",
    "        1. Assign 0 to chimp, 1 to dog, and 2 to human DNA.\n",
    "    3. Rename the original `class` column to `gene_family` for clarity.\n",
    "3. Create a new combined dataframe called `combined_df` with a balanced class distribution.\n",
    "    1. Find the species with the fewest DNA sequences.\n",
    "    2. Calculate the number of DNA sequences/rows that this species has and save it into the variable `num_samples_per_species`.\n",
    "    3. Randomly sample `num_samples_per_species` sequences from each species' dataframe. Make sure to set `random_state=SEED`!\n",
    "    4. Combine all the sampled DNA sequences into one dataframe.\n",
    "\n",
    "**Hints:**\n",
    "* Pandas's [`read_table` function](https://pandas.pydata.org/docs/reference/api/pandas.read_table.html) might be helpful for converting a txt file into a Pandas dataframe.\n",
    "* Make sure you set `random_state=SEED` when sampling rows from your dataframes.\n",
    "* You can use Pandas's [`concat` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html#pandas.concat) to combine your sampled DNA sequences. Make sure to specify which axis you are concatenating on and to set `ignore_index=True` so that the combined dataframe doesn't try to merge together rows from different dataframes that have the same index.\n",
    "* `combined_df` should have `3 * num_samples_per_species` rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JLdy-nQei8qE",
    "outputId": "efaad10b-8308-491b-fec1-3840d3db1b96",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "set_seed() # Resets the global seed state for reproducibility\n",
    "\n",
    "# Map of species name to class ID\n",
    "species_to_target = {\n",
    "    'chimpanzee': 0,\n",
    "    'dog': 1,\n",
    "    'human': 2\n",
    "}\n",
    "\n",
    "# Map of targets (class IDs) to species name\n",
    "target_to_species = {\n",
    "    target: species for species, target in species_to_target.items()\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: Create 3 dataframes of the chimpanzee, dog, and human DNA sequences\n",
    "chimpanzee_df = ...\n",
    "dog_df = ...\n",
    "human_df ...\n",
    "\n",
    "# TODO: Calculate the length of the dataframe that has the smallest number of DNA sequences\n",
    "num_samples_per_species = ...\n",
    "\n",
    "# TODO: Sample `num_samples_per_species` sequences from each of the 3 dataframes and combine them into 1 dataframe\n",
    "combined_df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_Y10p0bMk4qH"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "We'll take a peek at the first few characters of the chimpanzee, dog, and human DNA sequences.\n",
    "\n",
    "**Task:** Extract a DNA sequence from `combined_df` corresponding to a chimpanzee, dog, and human. Print the first 50 characters of each DNA sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "cuPFnFBLEe-o",
    "outputId": "64deb3e9-def9-4dae-cdf2-a234a0cce6ac",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "chimpanzee_dna = ...\n",
    "print(f\"First 50 DNA tags of chimpanzee sequence: {chimpanzee_dna[:50]}...\")\n",
    "\n",
    "dog_dna = ...\n",
    "print(f\"First 50 DNA tags of dog sequence: {dog_dna[:50]}...\")\n",
    "\n",
    "human_dna = ...\n",
    "print(f\"First 50 DNA tags of human sequence: {human_dna[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "j3yaTYzSlKVw"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4b: Tokenizing DNA into k-mers\n",
    "\n",
    "DNABERT-6 is designed to process 6-mers, which are overlapping, fixed-length sequences of 6 nucleotides each. Hence the \"6\" in its name!\n",
    "\n",
    "To tokenize DNA strings into 6-mers, we need to split the DNA nucleotides into overlapping groups of length 6.\n",
    "\n",
    "For instance, given the DNA sequence `ATGCGTACTAAG`, we can break it into  6-mers (each substring of length 6) like this:\n",
    "* Start at index 0: `ATGCGT`\n",
    "* Index 1: `TGCGTA`\n",
    "* Index 2: `GCGTAC`\n",
    "* Index 3: `CGTACT`\n",
    "* Index 4: `GTACTA`\n",
    "* Index 5: `TACTAA`\n",
    "* Index 6: `ACTAAG`\n",
    "\n",
    "Resulting 6-mers as a single string separated by spaces: `ATGCGT TGCGTA GCGTAC CGTACT GTACTA TACTAA ACTAAG`\n",
    "\n",
    "**Task:**\n",
    "1. Implement the `sequence_to_kmer` function, which converts a string into a space-separated string of k-mers. Note that the parameter `k` in the function can be varied to produced k-mers of different length.\n",
    "2. Apply the `sequence_to_kmer` function to all the rows in `combined_df` to generate 6-mers of all the DNA sequences. Save the 6-mers into a new column of the dataframe called `kmers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Kz4kVZ3tf4ap",
    "outputId": "d4a6e179-7ff6-432b-c418-0df583112ba9",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sequence_to_kmer(sequence, k=6):\n",
    "    \"\"\"\n",
    "    Converts a sequence into a space-separated string of k-mers.\n",
    "\n",
    "    A k-mer is a substring of length `k` from overlapping positions in\n",
    "    the input sequence. Returns all k-mers as a single string separated by spaces.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): The input sequence (e.g., DNA, RNA, or text) to tokenize into k-mers.\n",
    "        k (int, optional): Length of each k-mer. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        str: Space-separated string of k-mers.\n",
    "\n",
    "    Example:\n",
    "        >>> kmer_tokenize(\"ATGCGT\", k=3)\n",
    "        'ATG TGC GCG CGT'\n",
    "    \"\"\"\n",
    "    # TODO: Implement the kmer_tokenize function\n",
    "    ...\n",
    "\n",
    "# TODO: Apply `kmer_tokenze` to the DNA sequences and save it into a dataframe column called `kmers`\n",
    "combined_df['kmers'] = ...\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "print(combined_df.sample(5, random_state=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MdOw-N2XWGqt"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Loading a Pretrained DNABERT Model\n",
    "\n",
    "You can load in pretrained models (either with pretrained weights, meaning the model comes with values in their weight and bias matrices that have already been trained by other researchers/engineers/students, or without pretrained weights, meaning the model comes with randomly initialized weights) in many ways. For instance, the `transformers` library has 2 modules called `AutoTokenizer` and `AutoModel` for [loading pretrained models from HuggingFace](https://huggingface.co/docs/transformers/model_doc/auto):\n",
    ">```python\n",
    "># Load model from HuggingFace\n",
    ">model = AutoModel.from_pretrained('bert-base-uncased')\n",
    ">\n",
    "># Load vocabulary from HuggingFace\n",
    ">tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    ">```\n",
    "\n",
    "\n",
    "In PyTorch, you can [load pretrained image models](https://docs.pytorch.org/vision/main/models.html) by importing them from `torchvision.models`:\n",
    ">```python\n",
    ">from torchvision.models import resnet50, ResNet50_Weights\n",
    ">\n",
    "># Load ResNet50 with weights trained on ImageNet 1k\n",
    ">resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    ">\n",
    "># No weights - random initialization\n",
    ">resnet50(weights=None)\n",
    ">```\n",
    "\n",
    "We're going to load DNABERT-6 from HuggingFace: [DNABERT-6 Model Card](https://huggingface.co/zhihan1996/DNA_bert_6)\n",
    "\n",
    "**Note:** `revision` refers to a specific commit hash or version of the code to load the model from. We're going to use the commit from June 30, 2025 (the most recent one as of this homework's creation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "e1694810a15d407b9229c0b0f021443d",
      "de02e5cc45db4a659215ef7d251d6605",
      "694bb251f8a7465abf45ede2a97e0334",
      "b0f0596535d4413cbd3687dfcebefb6c",
      "14d61d2d34394e20a8af019d950e4dbc",
      "3210ae8b7d3042fe8ed41c21e0c5b7cd",
      "286669320bc847efb5d46fec206e6ec7",
      "71ee86ef180a4392b1770d20f724f671",
      "3a4abf07b34c45b78a248449a9720e21",
      "d02894f794584720bb341194d4e11b37",
      "a0e550d57b704f42aecf35714c25518e",
      "967fb3d27fe34bd9b3a4287a571069d7",
      "961d17e407e449d291d61a17e251d1df",
      "d433e50b0a884204a1f197423a775183",
      "1e9b42db72874dcda78f859337e7bdd8",
      "28eddf29a5064259a335df0dd300cc90",
      "b535afc4ced147b4a4a075d2d29d6c9d",
      "c8f712be35b94e268e535f23bdc60935",
      "5279e23b1c844945824aed79e7bd333d",
      "b4d63a29e2cf46019b94e986f5e4de05",
      "70ec29777ee04d3ba36df8607a2f11a0",
      "6f7cde7a15204dabbfc6e434bb892e59",
      "d43546af0c8b42418367ec61f95a4b6c",
      "0c1bc6a70ea44774b2dea16fb2ea5942",
      "b06348be40064caa8cb71ddf848584cd",
      "4af89811804345c88e4ac42d997c26a3",
      "cb0094bff733408299aad830ab12d52f",
      "e5a66a56316c4747813a0481b5ae3cb9",
      "9f523bb8c0904da7ad12a5617fa73670",
      "bc453eacacad40dc88ca7238904803e5",
      "65aa8c96eb9841ca9387e1bb2c404b32",
      "b7c029f52f6e4efc98006b899336336a",
      "12d33f3ae3104692af27c27a74bc308e",
      "4a3cc281c4e9448b8c5fd5a7c07b46bd",
      "db0d7df7ae0c478bba1d8cec2d97d968",
      "5f5e01e9e45c467095d9e256cba61836",
      "e7acf4d64f674e50851c60159f6b641a",
      "6506ecc0f2e34e54bfad5d7483a3dddd",
      "d9254e28efad4a20853024ad45462a06",
      "91a573cc20264230a27aa49962114870",
      "62523bc4f6c84c8a8b29eb21d0e05d45",
      "33a6daef02fd42f8b1ba7cae176ce94a",
      "869e85af581747b6b9a3bd7e5cc897fc",
      "f7ba7bc9d7934cfabdff496b1ae74629",
      "a90e22677c9c49589481bd8f8adf616f",
      "eaf9709e5a8b4f05a55ca702b69c1266",
      "e961336c34634eacb2566e474f43c975",
      "8dcfb66c68c04abcad3225d9cbc6d0a1",
      "f6c32b1d1c844436af7080c965809aa1",
      "ff37e8fc512c4d34a95cd99c65921581",
      "eb27f3b41b2b4df0aa59cb3e628146de",
      "feb68d056a294f649e449a6e83593db0",
      "7afd46d960f9484699d00fc37292afab",
      "8cf5c028e7c244f59ac63f464eec4073",
      "0a5754e4ca6141cbb5199815d6aae98f",
      "0e302db1f748433cbb5a9114f02f9d98",
      "4f2cf6d2e6a1415894ca3528e6d3eef4",
      "fb189504e3074bb48aec3d1f97aaf41b",
      "b4f7b6f87b1a44179c0a8b62038b2857",
      "84bd56739db5408b8e8bf3edb50de830",
      "f5510bca19554eb5a56c3cb5b7e1c043",
      "5351780c0a5c4677b94c138776a3d648",
      "a8d032247800426a81f6b10374cca3d6",
      "983e6390aeb4472bbeea166a09af9015",
      "e172abbd9fc246819adf50427e1f1e0c",
      "dc64e33b1add4abfabe3c1745558c131",
      "6bb9bd149fb241e39ebc23017bda5927",
      "76cfc720798845e881a1e32ad03f9667",
      "26ab0dc1d18640e29ac782e0d57b2e91",
      "d201d1ab425741e990ddbfc331be13e4",
      "5b726ba72821439eb731112fc115fdd7",
      "ae1aa712b3af42b8aefedb9e58454342",
      "0d4e3b26e633429889feac61a877e8a6",
      "4edf5309f0394a9897eca697de448302",
      "a100e88ae5654a4088bd58f5efac1805",
      "ea54e27462db4bc88afb3c1312b1f256",
      "fe2225bb6ee7416faa287d0161917fe0"
     ]
    },
    "id": "nTBK0JNOWKSs",
    "outputId": "f60ed2ad-58ab-4990-acd7-410efdf139f4"
   },
   "outputs": [],
   "source": [
    "dnabert_tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True, revision=\"c56e67ea5827e0ddc67ef059addcf71569b1216e\")\n",
    "dnabert_model = AutoModel.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True, revision=\"c56e67ea5827e0ddc67ef059addcf71569b1216e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3DBFESbR6mm"
   },
   "source": [
    "## From K-mers to DNABERT Embeddings\n",
    "\n",
    "Let's walk through the process of turning a k-mer string into model-ready input, and explain each step and output along the way.\n",
    "\n",
    "### Step 1: Tokenizing k-mers with the DNABERT Tokenizer\n",
    "\n",
    "To prepare a k-mer string like the ones we generated with the `sequence_to_kmer` function into tokens we can pass into DNABERT, we use our tokenizer.\n",
    "* `return_tensors='pt'`: This tells the tokenizer to return PyTorch tensors instead of plain lists. This is necessary because the model expects tensor inputs.\n",
    "* `max_length=512`: Sets the maximum number of tokens in the sequence. If your sequence is longer, it will be cut off at 512 tokens. We need to set this `max_length` parameter because this is the maximum number of tokens that DNABERT was designed to handle in one forward pass.\n",
    "* `truncation=True`: Ensures that if your sequence is longer than max_length, it will be truncated (cut off) rather than causing an error\n",
    "* `padding='max_length'`: This tells the tokenizer to pad all sequences to exactly max_length tokens. If your sequence is shorter than 512 tokens, special padding tokens are added at the end so that every input is the same length. Neural network models (like DNABERT) work most efficiently when all inputs in a batch are the same size. But real-world text (or DNA) sequences can have different lengths. To process batches of sequences at the same time, we need to \"fill up\" shorter sequences so that they all have the same number of tokens as the longest sequence in the batch. Tokenizers use a special token called a padding token at the end of shorter sequences to ensure  the tokenized versions of all the sequences in a batch have the same length.\n",
    "\n",
    "```\n",
    "tokens = tokenizer(kmer_string, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "```\n",
    "\n",
    "### Step 2: Understanding the Tokenizer Output\n",
    "The tokenizer returns a dictionary with several keys, but the most important are:\n",
    "* `input_ids`: The numerical IDs for each token in your input. Shape: `[batch_size, sequence_length]`(or `(1, 512)` for a single sequence).\n",
    "* `attention_mask`: A tensor of 1s and 0s indicating which tokens are real (1) and which are padding (0). The model uses the attention mask to \"mask out\" padding tokens so that it doesn't *attend* to padding tokens during processing, since the padding tokens don't actually hold useful information for us!\n",
    "\n",
    "### Step 3: Passing Inputs to the Model\n",
    "To get embeddings from DNABERT, we pass both   `input_ids` and `attention_mask` to the model:\n",
    "\n",
    "```\n",
    "outputs = model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "```\n",
    "\n",
    "### Step 4: Interpreting the Model Output\n",
    "Encoder-based transformer models from the HuggingFace Transformers library (like DNABERT-6) return a `transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions` object with several attributes. The most important for most tasks is:\n",
    "*  `last_hidden_state`: This is a tensor of shape `[batch_size, sequence_length, hidden_size]` (e.g., `(1, 512, 768)`).\n",
    "\n",
    "**What do these dimensions mean?**\n",
    "* `batch_size`: Number of sequences you processed at once (or 1 for a single example).\n",
    "* `sequence_length`: Number of tokens in your input (up to `max_length`).\n",
    "* `hidden_size`: The size of the embedding for each token (for DNABERT-6, this is 768).\n",
    "\n",
    "Each row in `last_hidden_state` corresponds to the embedding for a token in your input sequence. The very first token (position 0) is the special `[CLS]` token, whose embedding is often used to represent the whole sequence for classification tasks.\n",
    "\n",
    "Example:\n",
    "If you print `last_hidden_state.shape` and see `(10, 512, 768)`, it means:\n",
    "* 10 sequences in the batch\n",
    "* 512 tokens (including `[CLS]`, k-mers, `[SEP]`, and possibly padding)\n",
    "* Each token is represented by a 768-dimensional vector\n",
    "\n",
    "In the code block below, we demonstrate how you can tokenize a single k-mer sequence, pass the tokens into a DNABERT model, and extract the final hidden state!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZaoPoFcsehi5",
    "outputId": "38b68b46-6e90-4f3e-eb49-71ec8375be29"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# chimpanzee  #\n",
    "###############\n",
    "# Extract the k-mer for 1 chimpanzee DNA sequence\n",
    "chimpanzee_dna_kmer = combined_df.loc[combined_df['species'] == 'chimpanzee'].iloc[0, ]['kmers']\n",
    "\n",
    "# Tokenize the k-mer\n",
    "chimpanzee_tokens = dnabert_tokenizer(chimpanzee_dna_kmer, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "# You should see (1, 512)! Notice how the tokenizer generated exactly 512 tokens\n",
    "print(f\"Chimpanzee tokens shape: {chimpanzee_tokens['input_ids'].shape}\")\n",
    "\n",
    "# Printing the first 5 token IDs\n",
    "print(f\"First 5 tokens: {chimpanzee_tokens['input_ids'][0][:5]}...\")\n",
    "\n",
    "# Passing the input_ids and attention_mask to the DNABERT model\n",
    "chimpanzee_hidden_state = dnabert_model(input_ids=chimpanzee_tokens['input_ids'], attention_mask=chimpanzee_tokens['attention_mask']).last_hidden_state\n",
    "\n",
    "# You should see (1, 512, 768)\n",
    "print(f\"Chimpanzee hidden state shape: {chimpanzee_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9751IPXgEZ0"
   },
   "source": [
    "## Question 4c: Adding a Classification Head to DNABERT\n",
    "<div align=\"center\">\n",
    "<img src=\"https://i.imgur.com/w7L5mlg.jpeg\" alt=\"DNABERT Fine-tuning\" width=\"400\"/>\n",
    "<br>\n",
    "<caption>\n",
    "<a href=\"https://academic.oup.com/bioinformatics/article/37/15/2112/6128680\">\n",
    "Source: Bioinformatics - Ji, Zhou, Liu, and Davuluri\n",
    "</a>\n",
    "</caption>\n",
    "</div>\n",
    "\n",
    "BERT-based models (like BERT and DNABERT) are powerful at creating embeddings for sequences, but by default, they do not include a classification head‚Äîthat is, a layer that turns those embeddings into class predictions for tasks like sequence classification.\n",
    "\n",
    "To turn those embeddings into actual predictions (for example, classifying whether a movie review is positive or negative, or predicting what species our DNA sequences come from), we need to add extra PyTorch modules for classification (usually one or more linear layers that we will train specifically for our classification task).\n",
    "\n",
    "### How do we use BERT-based models for classification?\n",
    "\n",
    "Here‚Äôs the standard workflow for fine-tuning a BERT-based model like DNABERT on your own classification problem:\n",
    "\n",
    "1. Tokenize and encode the sequence into a list of integer Token IDs\n",
    "    1. In our case, we need to convert our DNA sequence to k-mers, then use the pretrained tokenizer to turn these k-mers into a list of token IDs.\n",
    "\n",
    "2. Pass the token IDs through the BERT model to get the hidden states for each token in the sequence.\n",
    "\n",
    "3. Extract the `[CLS]` embedding.\n",
    "The hidden state at the `[CLS]` position (usually `hidden_state[:, 0, :]` for each sequence) is designed to ‚Äúsummarize‚Äù the information from the whole sequence. This contains the features we'll use for classification!\n",
    "\n",
    "4. Feed the `[CLS]` embedding into a classification layer.\n",
    "Add a trainable layer (like `nn.Linear(hidden_size, num_classes)` in PyTorch) on top of the BERT model to transform the `[CLS] embedding` into logits for our classes.\n",
    "\n",
    "During training, we'll train our classification head on our labeled data so that it learns how to turn the embeddings our BERT encoder produces into meaningful class predictions!\n",
    "\n",
    "This approach is standard for all BERT-based models, whether we're working with DNA, RNA, or text in NLP! The classifier ‚Äúhead‚Äù learns to map the information-rich `[CLS]` embedding to your specific task.\n",
    "\n",
    "### Backbone and Classification Heads\n",
    "In the context of models like BERT that have been adapted for classification tasks, the **backbone** refers to the main part of the neural network outputs useful embeddings/features from the original inputs.\n",
    "* Its job is to take in raw input (like a DNA sequence or a sentence), processes it through many layers, and produces a rich, high-level representation (embedding) of the input.\n",
    "* Key point: The backbone is usually pretrained on a large dataset and is good at understanding general patterns in the data, but it doesn't make final predictions for your specific task\n",
    "\n",
    "The **classification head** is the part of our adapted model specifically responsible for making predictions.\n",
    "* Its job is to take the features produced by the backbone (often the `[CLS]` token embedding) and turn them into predictions for our task.\n",
    "* Key point: The classification head is what actually makes the final decision or prediction, using the information the backbone has extracted.\n",
    "\n",
    "In our case, our **backbone** is a DNABERT-6 model, and our **classification head** will be the additional linear layers we add to predict what species the DNA comes from.\n",
    "\n",
    "**Task:** Implement a `Classifier` that combines a DNABERT as our backbone and a classification head.\n",
    "1. Define a linear layer as the model's classifier in the `__init__` method.\n",
    "2. In the `forward` method:\n",
    "    1. Pass the `input_ids` and `attention_mask` to the DNABERT backbone, and extract the DNABERT output's last hidden state.\n",
    "    2. Get the embeddings for the `CLS` token for each item in the batch.\n",
    "    3. Pass the `CLS` token embeddings to the `classifier` to produce logits.\n",
    "    4. Return the logits. Recall that we usually don't apply softmax to logits inside the model itself, because PyTorch's cross-entropy loss function will take the softmax of our logits when computing the loss.\n",
    "\n",
    "**Hints:**\n",
    "* When instantiating an `nn.Linear` layer, you will need to specify an `in_features` parameter and an `out_features` parameter. How does the number of dimensions in each embedding outputted by DNABERT relate to what the `in_features` dimension of the linear layer should be? How does the number of classes we want to make predictions for relate to what the `out_features` dimension of the linear layer should be?\n",
    "* The last hidden state of the backbone's output can be accessed with the `last_hidden_state` attribute\n",
    "* You can use slicing to access all the `CLS` embeddings for each item in the batch dimension at once. Recall that the `CLS` token's embeddings are located at the first index for every item in the batch. Here is a [helpful tutorial about slicing tensors from GeeksForGeeks](https://www.geeksforgeeks.org/python/how-to-slice-a-3d-tensor-in-pytorch/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTSHoFC0hp_V",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class DNAClassifier(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "    super().__init__()\n",
    "    self.backbone = AutoModel.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True)\n",
    "    self.classifier = ...\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    # TODO: Pass the input_ids and attention_mask\n",
    "    last_hidden_state = ...\n",
    "\n",
    "    # TODO: Get the embedding of the [CLS] token\n",
    "    cls_embeddings = ...\n",
    "\n",
    "    # TODO: Pass the cls_embeddings into the classifier to get class predictions\n",
    "    logits = ...\n",
    "    return logits\n",
    "\n",
    "  def print_params(self):\n",
    "    for name, param in self.backbone.named_parameters():\n",
    "      print(f\"Name: {name}\\tparameter shape: {param.shape}\\trequires grad: {param.requires_grad}\")\n",
    "    for name, param in self.classifier.named_parameters():\n",
    "      print(f\"Name: {name}\\tparameter shape: {param.shape}\\trequires grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AB5AAZgXrNLh"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4d: Custom DNA Dataset\n",
    "\n",
    "Let's create a custom DNA Dataset that will return the DNA kmers as tokenized inputs our DNABERT model can use for classification.\n",
    "\n",
    "Recall that when we create our own Dataset that subclasses from PyTorch's `Dataset` parent class, we need to implement 2 methods: `__len__` returns the number of samples in the dataset, and `__getitem__` returns one sample from the dataset.\n",
    "\n",
    "You might notice that we've defined a `return_targets` parameter in the init method. When we use our custom dataset for training data, we will use `return_targets` so that our dataset also returns the true target labels. But this way, we can also use our custom dataset for test sets when we won't have provided targets/labels ahead of time (like when we make predictions on test data for the Kaggle competition later on!).\n",
    "\n",
    "**Task:**\n",
    "1. Fill in the `__len__` method.\n",
    "2. Fill in the `__getitem__` method. In the `__getitem__` method, you should:\n",
    "    1. Get 1 k-mer (and its corresponding target label if `return_targets` is True).\n",
    "    2. Pass the k-mer into the tokenizer to get its tokens. Don't forget to specify the `return_tensors`, `max_length`, `truncation`, and `padding` parameters!\n",
    "    3. Since the tokenizer will return a batched output but `__getitem__` will return data for just a single sample, you should extract the `input_ids`from the tokenizer output and reshape it from shape `(1, 512)` into `(512)`.\n",
    "    4. Likewise, extract `attention_mask` from the tokenizer output and reshape it from `(1, 512, 512)` into `(512, 512)`.\n",
    "    5. If `return_targets` is True, cast the label into a `torch.Tensor` with `dtype=torch.long`.\n",
    "    6. Return a dictionary with the following keys: `input_ids`, `attention_mask` (and `targets` if `return_targets` is True).\n",
    "\n",
    "**Hints:**\n",
    "* After tokenizing, use [`squeeze(...)`](https://docs.pytorch.org/docs/stable/generated/torch.squeeze.html) to remove the batch dimension from the tensors.\n",
    "\n",
    "**Notes about batching:**\n",
    "* When we use the DNABERT tokenizer with `return_tensors='pt'`, it returns tensors with a batch dimension, even if you only tokenize a single sequence.\n",
    "    * For example, `input_ids` will have shape `(1, max_length)` and `attention_mask` will have shape `(1, max_length, max_length)`. Notice how the first dimension, which corresponds to the batch size, has a value of 1 to represent that we tokenized just a single sequence!\n",
    "* When we use a DataLoader to create batches, it will automatically stack multiple samples from the Dataset together along a new batch dimension. If you leave the batch dimension in each sample, you'll end up with an extra, unwanted dimension (e.g., (batch_size, 1, max_length) instead of (batch_size, max_length)). So if we don't remove the batch dimension in the `__getitem__` method of our Dataset, we'll get shape mismatches during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxxPXjn5Zjq2",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "  def __init__(self, kmers: list[str], targets: list[int] = None, tokenizer=None, return_targets=True):\n",
    "    super().__init__()\n",
    "    self.kmers = kmers\n",
    "    self.targets = targets\n",
    "    self.return_targets = return_targets  # Whether to return targets or not\n",
    "    self.tokenizer = tokenizer if tokenizer is not None else AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True, revision=\"c56e67ea5827e0ddc67ef059addcf71569b1216e\")\n",
    "\n",
    "  def __len__(self):\n",
    "    # TODO: Return the number of samples in the dataset\n",
    "    ...\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    # TODO: Get the k-mer at the requested index\n",
    "    ...\n",
    "\n",
    "    # TODO: If return_targets is True, get the target at the requested index and convert to torch.Tensor with dtype=torch.long\n",
    "    ...\n",
    "\n",
    "    # TODO: Tokenize the k-mer using the tokenizer. Don't forget to specify return_tensors, max_length, truncation, and padding!\n",
    "    ...\n",
    "\n",
    "    # TODO: Extract and reshape the input_ids\n",
    "    ...\n",
    "\n",
    "    # TODO: Extract and reshape the attention_mask\n",
    "    ...\n",
    "\n",
    "    # TODO: Return a dictionary with the input_ids, attention_mask, and target (if return_targets is True)\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D07SLJVO8nba"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4e: Preparing Training and Validation Dataloaders\n",
    "\n",
    "We'll use the `kmers` column from `combined_df` as our input features and the `target` column as our target labels.\n",
    "\n",
    "There are a lot of DNA sequences from our txt files, but for this homework, we'll train on a more manageable subset of 1000 DNA sequences!\n",
    "\n",
    "**Task:**\n",
    "1. Sample 1000 rows from `combined_df`.\n",
    "2. Use scikit-learn's `train_test_split` to divide the sequences and targets from the sampled subset of data into a training set (80%) and validation set (20%).\n",
    "    1. Make sure to use the `stratify` parameter, which ensures that both the training set and validation set have similar proportions of all the classes! This helps the model learn how to predict all the classes more easily, and prevents us from accidentally training the model on one kind of skewed class distribution and validating it on another kind of skewed class distribution.\n",
    "3. Use the custom `DNADataset` class we defined earlier to create a training set and validation set\n",
    "4. Make training and validation DataLoaders\n",
    "    1. Set `batch_size=32` for both DataLoaders\n",
    "    2. Set `shuffle=True` for the training DataLoader. This randomizes the order of the data each epoch so the model doesn't memorize it.\n",
    "    3. For validation, use `shuffle=False` so the results are always in the same order for comparison.\n",
    "\n",
    "**Hint:**\n",
    "* The `kmers` column from the `combined_df` dataframe should be the input features, and the `target` column should be the target labels.\n",
    "* You can learn more about scikit-learn's [`train_test_split` function here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QyH9mor8qo2",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "set_seed() # Resets the global seed state for reproducibility\n",
    "# TODO: Sample 1000 DNA sequences from `combined_df`. Make sure to set the random_state!\n",
    "...\n",
    "\n",
    "# TODO: Use train_test_split to create training and validation splits\n",
    "...\n",
    "\n",
    "# TODO: Create a training and validation set using the DNADataset class we implemented\n",
    "train_set = ...\n",
    "val_set = ...\n",
    "\n",
    "# TODO: Create training and validation DataLoaders.\n",
    "# Set shuffle=True for the training dataloader and shuffle=False for the validation dataloder\n",
    "train_dataloader = ...\n",
    "val_dataloader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GwLe4O-mjuCq"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4f: Write a Training Loop for the DNA Classifier\n",
    "\n",
    "We've set up the basic scaffolding for a training loop to train a DNA Classifier model! Now it's time to fill in the missing pieces so the function can actually perform training and validation in each epoch.\n",
    "\n",
    "**Task:** Complete the loop to train and evaluate a DNA classifier.\n",
    "\n",
    "For each batch inside the training loop:\n",
    "1. Move the training input data and targets to the specified device.\n",
    "2. Reset the optimizer gradients to zero so gradients from the previous batch don't accumulate.\n",
    "3. Run a forward pass: Pass the `input_ids` and `attention_mask` through the model to produce logits.\n",
    "4. Calculate the loss between the model's predictions and the true labels using the `criterion` (loss function).\n",
    "5. Compute the gradients for the model's parameters by backpropagating from the loss.\n",
    "6. Update the model parameters by taking one step of the optimizer.\n",
    "7. Track training statistics.\n",
    "    1. Add the current loss to the running total in `train_loss`\n",
    "    2. Count how many samples the model correctly predicted, and add it to `train_correct`.\n",
    "\n",
    "For each batch inside the validation loop:\n",
    "\n",
    "8. Move the validation input data and targets to the device.\n",
    "9. Do a forward pass through the model to get predictions.\n",
    "10. Compute the loss on the validation data\n",
    "11. Track validation metrics\n",
    "    1. Calculate loss and track the number of correct validation predictions.\n",
    "\n",
    "**Hints:**\n",
    "* Make sure you pass the logits and the true targets to the criterion in the right order (logits first, then true targets)!\n",
    "* The model's output for each sample is a set of logits that tells us how confident the model is about the sample belonging to each class. More concretely, for each sample in a batch, the model outputs a vector of length equal to the number of classes (e.g., [logit_0, logit_1, logit_2] for class 0, class 1, and class 3). The highest logit in this vector corresponds to the class the model predicts for that sample.\n",
    "    * To get the predicted class, you can use `torch.max` and specify which dimension you want to find the max over! `torch.max` will return two values: the max, and the argmax (i.e. the index where the max occurs). What is the connection betweeh the argmax and the predicted class?\n",
    "* Since `train_loss` and `train_correct` are floats and integers respectively, you can't directly add a tensor to them. To add a 1D scalar tensor to a float/integer in Python, you need to call `.item()` on the tensor.\n",
    "\n",
    "**Notes: How we calculate average loss and accuracy**\n",
    "* When training a model, we generally track the average loss and average accuracy to analyze how well the model is performing per sample and compare results across different epochs and experiments.\n",
    "\n",
    "**Loss Calculation**\n",
    "* During each epoch, we add the average loss computed over the batch's items to a running total. Each batch's loss is an average over the samples in that batch, so dividing by the number of batches at the end of the epoch gives us the average loss per batch for the whole epoch.\n",
    "\n",
    "**Accuracy Calculation**\n",
    "* During each epoch, we count the total number of correct predictions the model makes for all samples in the dataset. Thus, at the end of the epoch we should divide by the total number of samples in the dataset to calculate what fraction of all the samples in the dataset the model predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JokdrVfsc-V",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_dna_classifier(model, optimizer, criterion, device, num_epochs, train_dataloader, val_dataloader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: the model to train\n",
    "        optimizer: the optimizer to use\n",
    "        criterion: the loss function to use\n",
    "        num_epochs: the number of epochs to train for\n",
    "        train_dataloader: the dataloader for the training set\n",
    "        val_dataloader: the dataloader for the validation set\n",
    "    Returns:\n",
    "        train_losses: a list of training losses for each epoch\n",
    "        val_losses: a list of validation losses for each epoch\n",
    "        train_accuracies: a list of training accuracies for each epoch\n",
    "        val_accuracies: a list of validation accuracies for each epoch\n",
    "    \"\"\"\n",
    "    # === SETUP ===\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store metrics across epochs\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # === EPOCH LOOP ===\n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRAINING PHASE ===\n",
    "        model.train() # Set model to training mode\n",
    "\n",
    "        # Initialize metrics for this epoch\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        # === INNER LOOP (iterate over training batches) ===\n",
    "        for batch in train_dataloader:\n",
    "            # TODO 1: Move the data and the targest to device\n",
    "            input_ids = ...\n",
    "            attention_mask = ...\n",
    "            targets = ...\n",
    "\n",
    "            # TODO 2: Reset gradients\n",
    "            ...\n",
    "\n",
    "            # TODO 3: Forward pass: pass inputs to model\n",
    "            logits = ...\n",
    "\n",
    "            # TODO 4: Compute loss\n",
    "            loss = ...\n",
    "\n",
    "            # TODO 5: Backward pass/compute gradients\n",
    "            ...\n",
    "\n",
    "            # TODO 6: Update parameters\n",
    "            ...\n",
    "\n",
    "            # TODO 7: Track training metrics for this epoch\n",
    "            ...\n",
    "            _, preds = ...\n",
    "            ...\n",
    "\n",
    "        # === END OF INNER LOOP ===\n",
    "\n",
    "        # Compute average training metrics for the epoch\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc = train_correct / len(train_dataloader.dataset)\n",
    "\n",
    "        # Append this epoch's training metrics to history\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Training loss = {train_loss}\\tTraining accuracy = {train_acc}\")\n",
    "\n",
    "        # === END OF TRAINING PHASE ===\n",
    "\n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval() # set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # === INNER LOOP (iterate over validation batches) ===\n",
    "            for batch in val_dataloader:\n",
    "                # TODO 8: Move data to device\n",
    "                input_ids = ...\n",
    "                attention_mask = ...\n",
    "                targets = ...\n",
    "\n",
    "                # TODO 9: Forward pass only\n",
    "                logits = ...\n",
    "\n",
    "                # TODO 10: Compute loss\n",
    "                loss = ...\n",
    "\n",
    "                # TODO 11: Track validation metrics\n",
    "                ...\n",
    "                _, preds = ...\n",
    "                ...\n",
    "\n",
    "            # === END OF INNER LOOP ===\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc = val_correct / len(val_dataloader.dataset)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Epoch {epoch + 1}: Validation loss = {val_loss}\\tValidation accuracy = {val_acc}\")\n",
    "\n",
    "        # === END OF VALIDATION PHASE ===\n",
    "\n",
    "    # === END OF EPOCH LOOP ===\n",
    "\n",
    "    # Return history\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "n8NueJhU6bJR"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4g: Train your DNA Classifier!\n",
    "\n",
    "**Task:** Initialize and train a DNA Classifier using the `train_dna_classifier` function and the training and validation DataLoaders.\n",
    "1. Initialize an instance of the `DNAClassifier` class we implemented in question 4c.\n",
    "2. Use [`AdamW`](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html) as the optimizer.\n",
    "3. Use [`CrossEntropyLoss`](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as the criterion\n",
    "4. Train your DNA Classifier for 5 epochs.\n",
    "\n",
    "Your final training accuracy after 5 epochs should be above 0.5!\n",
    "\n",
    "**Hints:**\n",
    "* Recall that the `DNAClassifier` class takes in a `num_classes` parameter. In our case, we are trying to classify a DNA as either a chimpanzee, dog, or human. How many classes are we making predictions for?\n",
    "* Don't forget to pass the parameters of your DNA Classifier model into the optimizer. You can access all the parameters of any object that inherits from the `nn.Module` subclass by calling `parameters()`.\n",
    "* **Play around with what learning rate works best for your AdamW optimizer!** You can tell if your learning rate is too high or too low by observing if the training metrics printed every epoch are improving. For example, we recommend starting with learning rates in the range `1e-4` to `1e-3`, and increasing the learning rate by factors of 2x or 10x to see if it yields improved performance.\n",
    "\n",
    "**Notes:**\n",
    "* You may notice that the validation accuracy lags below the training accuracy. That is okay! Since we are using just a small subset of our data (only 1000 samples total!), our model might be overfitting to the training set. In practical experiments, you'll probably be using much bigger datasets, but for instructive purposes, we'll use this smaller set.\n",
    "* Make sure you are using an **accelerator** such as a **Colab GPU** to train your model! For reference, it took me (Deena) about 7 minutes to train my model on Google Colab's T4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "referenced_widgets": [
      "bd1dc11c3b8c4571b7fc3419e0cfd4f3",
      "a543efcb4bce4b848626be037f7a1b07",
      "af843f8f6ec74644986be6c931c366a2",
      "ad1cbfdfabd340fa9d6c595dae6b6c25",
      "73d9d78819074a1b91e1dc91a9274d91",
      "4d1900b7e856419e81854b7356573b4b",
      "8c25559e68ff4717b3bc7277898a3038",
      "1757fc6eb4ff46ddb5366cbc1f12cccf",
      "8f0473e6c6984dd38dacf9f686a0ebcd",
      "338a6b71370241dfaf433e0d5faf878b",
      "54869bb1f41e430d962f1a1c150f867f"
     ]
    },
    "collapsed": true,
    "id": "VjgiIPsJb-jb",
    "outputId": "276a0e82-5081-4587-ddd7-e896cac36c2f",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Instantiate a DNAClassifier\n",
    "dna_classifier = ...\n",
    "\n",
    "# TODO: Define the optimizer\n",
    "optimizer = ...\n",
    "\n",
    "# TODO: Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Train your DNA classifier for 5 epochs!\n",
    "dna_train_losses, dna_val_losses, dna_train_accuracies, dna_val_accuracies = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lDgDLZTarxKR"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4h: Visualize the DNA Classifier's Training Performance\n",
    "\n",
    "**Task:** Implement the `plot_metrics` function below to plot a model's training loss, training accuracy (if provided), validation loss, and validation accuracy (if provided). Then, call the function with the **DNA Classifier's** training loss, training accuracy, validation loss, and validation accuracy to **visualize the DNA Classifier's performance.**\n",
    "* The loss/accuracy should be on the y-axis, and the number of epochs should be on the x-axis.\n",
    "* You can use Matplotlib or Plotly.\n",
    "* When only the training loss and validation loss are provided, your function should produce 2 total plots in total: (1) training loss, and (2) validation loss.\n",
    "* When all 4 metrics are provided, your function should produce 4 total plots in total: (1) training loss, (2) training accuracy, (3) validation loss, and (4) validation accuracy.\n",
    "\n",
    "**Hint:** You can use the `plot_metrics` function we implemented in HW 4 Part 1, Question 1e of this assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1317
    },
    "id": "ty5dMyXJqluK",
    "outputId": "4da867c3-85e7-4d65-a207-faaecb442cd9",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_metrics(train_losses, val_losses, train_accuracies=None, val_accuracies=None, num_epochs=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plots the training loss, training accuracy, validation loss, and validation accuracy.\n",
    "    Args:\n",
    "        train_losses: list of training losses\n",
    "        val_losses: list of validation losses\n",
    "        train_accuracies: list of training accuracies\n",
    "        val_accuracies: list of validation accuracies\n",
    "        num_epochs: number of epochs\n",
    "        title: title of the plot\n",
    "    \"\"\"\n",
    "\n",
    "# TODO: Plot your DNA classifier's loss and accuracy curves on the training data and the validation data.\n",
    "# You should have 4 plots total!\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MH3ZLhgKEZu1"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4i: DNA Classification Kaggle Competition\n",
    "\n",
    "Let's take the DNA classification model we just trained and apply it to our class Kaggle competition! Your homework folder should include a file called `dna_test.txt`, which contains 1377 DNA sequences. In the cell below, we've written a helper function for you that loads the `dna_test.txt` file, makes predictions for each DNA sequence, and saves the predictions to a csv file called `dna_test_predictions.csv`.\n",
    "\n",
    "You should then submit your csv file to the Kaggle competition: [Fa25 CS189 HW4 - DNABERT](https://www.kaggle.com/competitions/fa-25-cs-189-hw-4-competition)\n",
    "\n",
    "**Expected Output:**\n",
    "* Your `dna_test_predictions.csv` file should have 2 columns:\n",
    "    * `ID`: ID of the DNA sequence from the provided test set\n",
    "    * `class`: the predicted species of the DNA sequence (0 for chimpanzee, 1 for dog, 2 for human).\n",
    "\n",
    "**Task:**\n",
    "* Train an instance of a DNA classifier. You have lots of freedom to try changing different parameters, such as how many epochs you train your model for, optimizer choice, learning rate, momentum values, etc.\n",
    "    * You can also reuse your DNA classification model you trained in question 4g, but we encourage you to explore how you can improve your base model. The space of possibilities to try out is immense!\n",
    "* Call `generate_dna_test_predictions` with your model to generate a `dna_test_predictions.csv` file.\n",
    "* Submit your predictions to the DNA classification Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVIKzzezNaK4"
   },
   "outputs": [],
   "source": [
    "def generate_dna_test_predictions(model) -> None:\n",
    "    \"\"\"\n",
    "    Generates predictions for the DNA test set using the specified model.\n",
    "\n",
    "    Loads the DNA test set from 'dna_test.txt', processes DNA sequences into k-mers,\n",
    "    creates a DNADataset, and runs the model to obtain predictions for each sequence.\n",
    "    Predictions are saved to 'dna_test_predictions.csv' with columns 'ID' and 'class'.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained PyTorch model for DNA sequence classification.\n",
    "\n",
    "    Outputs:\n",
    "        dna_test_predictions.csv: CSV file with index column 'ID' and a 'class' column\n",
    "        containing the predicted class for each dna sample in the test set.\n",
    "    \"\"\"\n",
    "    # Load the DNA test set\n",
    "    dna_test_df = pd.read_csv('dna_test.txt', sep='\\t')\n",
    "    dna_test_df.columns = ['ID', 'sequence']\n",
    "\n",
    "    # Convert sequences to k-mers (space-separated strings for tokenizer)\n",
    "    dna_kmers = dna_test_df['sequence'].apply(lambda seq: sequence_to_kmer(seq, k=6))\n",
    "\n",
    "    # Create test dataset using DNADataset\n",
    "    test_dataset = DNADataset(dna_kmers.tolist(), return_targets=False)\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)): # Iterate through all samples in our Dataset\n",
    "            sample = test_dataset[i]\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)  # Add batch dimension: shape (1, seq_len of 512)\n",
    "            attention_mask = sample['attention_mask'].unsqueeze(0).to(device)  # Add batch dimension: shape (1, seq_len of 512)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            prediction = torch.argmax(outputs, dim=1).item()\n",
    "            predictions.append(prediction)\n",
    "\n",
    "    # Create predictions dataframe\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ID': dna_test_df['ID'],\n",
    "        'class': predictions\n",
    "    }).set_index('ID')\n",
    "\n",
    "    predictions_df.to_csv('dna_test_predictions.csv', index=True) # Generates a csv with 2 columns: ID and class\n",
    "    print(\"Submissions saved to dna_test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOpmaAEOEZu2",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Optional TODO: Instantiate and train your best DNA classification model!\n",
    "\n",
    "# TODO: Use your best DNA classifier to make a prediction\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IemDQ4u8u0cR"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Subpart 2: Matrices are All You Need - ConvNeXt Sound Classification\n",
    "\n",
    "Similar to how we can use transformers to encode and decode anything that can be represented as a sequence of numerical tokens, we'll now also explore how creating clever matrix representations can let computer vision models tackle fascinating problems.\n",
    "\n",
    "At first glance (or first listen), sound might seem like a very different modality from the image data that computer vision models work with. However, by thinking outside the box, we can actually transform the task of sound classification into the task of image classification.\n",
    "\n",
    "While we're only going to be exploring how to apply computer vision models to sound classification, the possibilities are endless: imagine turning network signals into an graph to detect cybersecurity anomalies, or using meterology data to predict hurricane paths!\n",
    "\n",
    "First, we'll load the UrbanSound8k dataset.\n",
    "\n",
    "## UrbanSound8k\n",
    "\n",
    "[UrbanSound8k Kaggle Dataset](https://www.kaggle.com/datasets/chrisfilo/urbansound8k?select=UrbanSound8K.csv)\n",
    "\n",
    "The UrbanSound8K dataset consists of 8,732 short audio clips, each lasting up to 4 seconds, collected from real-world urban environments. Every clip is labeled with one of 10 common urban sound classes, such as car horn, dog bark, siren, jackhammer, and more. The dataset also includes a csv with additional metadata about each sound file,including the sound class, the recording fold, and additional details to support supervised learning and evaluation.\n",
    "\n",
    "\n",
    "The dataset follows a specific naming convention:\n",
    "Each audio file is named as\n",
    "`[fsID]-[classID]-[occurrenceID]-[sliceID].wav`\n",
    "where:\n",
    "* `fsID`: a unique identifier for the original recording,\n",
    "* `classID`: an integer representing the sound class (0 for air_conditioner, 1 for car horn, etc.),\n",
    "* `occurrenceID`: which instance of that sound in the field recording,\n",
    "* `sliceID`: identifies the particular slice within the occurrence.\n",
    "\n",
    "For example, `100032-3-0-0.wav` is the first slice `(0)` of the first occurrence `(0)` of the sound class `3` (which corresponds to ‚Äúdog bark‚Äù) from original recording ID `100032`.\n",
    "\n",
    "For our purposes, we'll only need the `classID` substring from the file names!\n",
    "\n",
    "### What is Cross-Validation?\n",
    "The UrbanSound8K dataset is organized into 10 separate ‚Äúfolds‚Äù, which are different partitions of the data. Each clip in the dataset is assigned to one of these folds (from 1 to 10), which helps support **cross-validation** experiments for more robust model evaluation. In typical practice, a few folds are used for training, and the remaining ones for validation or testing, rotating which folds are used for each split.\n",
    "\n",
    "**Cross-validation** is a technique used to reliably estimate how well a machine learning model will perform on new, unseen data. Instead of training and testing our model on just one split of the data, we divide our dataset into several parts (called folds) and rotate using different folds for training and testing. This way, we get multiple performance measurements that can be averaged for a fairer estimate.\n",
    "* Suppose we have our dataset split into 10 folds (as in UrbanSound8K). In each \"round\" of cross-validation, we use 9 folds for training and 1 fold for testing. We repeat this ten times, each time with a different fold held out as your test set.\n",
    "* This ensures every example gets used once for testing and multiple times for training.\n",
    "\n",
    "For this assignment, we‚Äôll only use the audio clips from fold 1. This keeps things simple, enabling faster prototyping and easier debugging as you work through the fundamentals of urban sound classification.\n",
    "\n",
    "Acknowledgements: J. Salamon, C. Jacoby and J. P. Bello, [\"A Dataset and Taxonomy for Urban Sound Research\"](https://dl.acm.org/doi/10.1145/2647868.2655045), 22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "V-DUICx2p96m"
   },
   "outputs": [],
   "source": [
    "# Unzip the dataset. This may take a few minutes!\n",
    "!unzip -q urbansound8k_fold1_train.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STCJwYBr3pnu"
   },
   "source": [
    "Let's listen to one of the audio clips we just downloaded. According to the UrbanSound8k dataset's description, these are the mappings of the `classID`s to the sound's class:\n",
    ">```python\n",
    ">class_id_to_sound = {\n",
    ">    0: \"air_conditioner\",\n",
    ">    1: \"car_horn\",\n",
    ">    2: \"children_playing\",\n",
    ">    3: \"dog_bark\",\n",
    ">    4: \"drilling\",\n",
    ">    5: \"engine_idling\",\n",
    ">    6: \"gun_shot\",\n",
    ">    7: \"jackhammer\",\n",
    ">    8: \"siren\",\n",
    ">    9: \"street_music\"\n",
    "> }\n",
    ">```\n",
    "\n",
    "Feel free to change the path of the audio file to listen to different clips!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "_EznXLNXxQlq",
    "outputId": "7219f688-d427-464b-ac73-6486c4618940"
   },
   "outputs": [],
   "source": [
    "audio_file = '/content/drive/MyDrive/cs189/hw/hw4/data/fold1/101415-3-0-2.wav'\n",
    "\n",
    "if IS_COLAB:\n",
    "    from IPython.display import Audio, display\n",
    "    display(Audio(audio_file, autoplay=False))\n",
    "else:\n",
    "    import pygame\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(audio_file)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wWjvi1v48P1"
   },
   "source": [
    "## Creating Spectrograms with Torchaudio\n",
    "\n",
    "Besides being an awesome Python library for building neural networks, PyTorch also has an official audio processing library called **Torch audio** that comes with handy tools for loading, transforming, and analyzing audio data in deep learning applications. **Torchaudio** is designed to work seamlessly with PyTorch and offers GPU acceleration for audio processing tasks.\n",
    "\n",
    "The `torchaudio.transforms` module contains common audio feature extraction functions such as spectrograms. These trasnforms can convert raw waveforms into numeric represetnations suitable for neural networks.\n",
    "\n",
    "The `torchaudio.transforms.Spectrogram` transform converts a raw audio waveform (a 1D array of amplitude values) into a spectrogram. This transformation is useful because it turns sound into an image-like format, which can be processed by image-based machine learning models.\n",
    "\n",
    "**Spectrograms** are a 2D representation of how the frequency content of the audio changes over time.\n",
    "* The spectrograms are generated by applying the **Short-Time Fourier Transform (STFT)** to overlapping windows of the audio signal. The result is a matrix where:\n",
    "    * The x-axis represents time (split into small chunks called time bins)\n",
    "    * The y-axis represents frequency (split into frequency bins)\n",
    "    * The values in the matrix show the strength (amplitude) of each frequency at each time\n",
    "* The `n_fft` parameter stands for \"number of FFT points\". It controls the size of the window (in samples) used for each Fourier Transform when creating the spectrogram. In other words, it determines how many audio samples are analyzed at a time to compute the frequency content. For example, `n_fft=1024` means each window of 1024 samples is transformed to analyze its frequency content. The choice of `n_fft` is a trade-off between how precisely you want to see changes in frequency versus changes in time.\n",
    "\n",
    "\n",
    "In the following code:\n",
    "1. We load an audio file using `torchaudio.load`, which returns the waveform and the rate at which the audio was sampled.\n",
    "2. Some of the audio clips are stereo (meaning they have 2 audio channels). In those cases, we average the 2 chanenls into a single (mono) channel for easier processing.\n",
    "3. Then, we use the spectrogram transform to extract a spectrogram of the audio.\n",
    "4. Finally, we plot the spectrogram using `matplotlib`\n",
    "    1. Note: The raw values in the spectrogram represent the power (or energy) of each frequency at each time. However, these power values can vary by several orders of magnitude; some frequencies are much louder than others, and the differences can be huge. Taking the log of the power values is a common practice in signal processing to make the scale easier to interpret. Each step in the y-axis corresponds to doubling in power. This is why in the code, we plot `spec.log2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "gnxjxxmsytBK",
    "outputId": "3e94c361-2435-4cf2-ab1c-25aa572cfc00"
   },
   "outputs": [],
   "source": [
    "spectrogram_transform = torchaudio.transforms.Spectrogram(n_fft=1024, normalized=True)\n",
    "\n",
    "class_id_to_sound = {\n",
    "    0: \"air_conditioner\",\n",
    "    1: \"car_horn\",\n",
    "    2: \"children_playing\",\n",
    "    3: \"dog_bark\",\n",
    "    4: \"drilling\",\n",
    "    5: \"engine_idling\",\n",
    "    6: \"gun_shot\",\n",
    "    7: \"jackhammer\",\n",
    "    8: \"siren\",\n",
    "    9: \"street_music\"\n",
    "}\n",
    "\n",
    "audio_file = '/content/drive/MyDrive/cs189/hw/hw4/data/fold1/101415-3-0-2.wav'\n",
    "\n",
    "try:\n",
    "    # Read WAV file\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "    # Converts stero to mono by averaging channels. Shape becomes [1, num_samples]\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Transform the waveform into a spectrogram\n",
    "    spec = spectrogram_transform(waveform) # torch.Tensor of shape [1, freq_bins, time_bins]\n",
    "\n",
    "    # Print the shape of the spectrogram\n",
    "    print(f\"Spectrogram shape: {spec.shape}\") # shape [num_frequencies, num_time_bins]\n",
    "\n",
    "    # Visualize the spectrogram\n",
    "    plt.figure()\n",
    "    plt.imshow(spec.log2()[0, :, :].numpy(), aspect='auto', origin='lower') # origin = 'lower' sets input[0, 0] at bottom left\n",
    "    plt.title(f\"Spectrogram: {audio_file.split('/')[-1]}\")\n",
    "    plt.xlabel(\"Time bins\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {audio_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FZ6vCYNs-Bc"
   },
   "source": [
    "## Question 5a: Creating a Custom Dataset of our Spectrograms\n",
    "\n",
    "We will implement a custom PyTorch Dataset for audio spectrogram classification. Since our audio files that we will be using for training are stored as `.wav` files inside a folder, this dataset will load .wav audio files, convert each to a spectrogram image, and extract the class label from the file name.\n",
    "\n",
    "**Task:** Implement the `__len__` and `__getitem__` methods of the SpectrogramDataset\n",
    "1. Implement the `__len__` method, which returns the total number of samples in the Dataset.\n",
    "2. Implement the `__getitem__` method.\n",
    "    1. If `return_targets` is `True`, parse the `filename` to get the requested sample's target class. Convert this into a torch.Tensor with `dtype=torch.long`. We will use `return_targets` when creating a training dataset for our model, but when we make test predictions on a test set, we won't have provided targets/labels to instantiate our Dataset with.\n",
    "3. Load the waveform using `torchaudio.load`.\n",
    "4. If the audio has 2 stereo channels, average them into a single-channel mono waveform.\n",
    "5. Generate a spectrogram of the waveform using the class's `spectrogram_transform`.\n",
    "6. Computer vision models often expect 3-channel input images. The spectrogram only has 1 channel, but we can copy the spectrogram into 3 different channels. Repeat the spectrogram across the first (channel) dimension into a new tensor with shape `(3, H, W)`.\n",
    "7. If provided, apply additional transformations to the spectrogram.\n",
    "8. Return the processed spectrogram image and corresponding label.\n",
    "\n",
    "**Hints:**\n",
    "1. Python's [`split` function for strings](https://www.w3schools.com/python/ref_string_split.asp) might be helpful for extracting the class ID from the file name.\n",
    "2. Remember to cast your target to `dtype=torch.long`, which is the datatype that PyTorch loss functions expect for the true labels!\n",
    "3. You can use [`mean`](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean) to take the average of a `torch.Tensor` over a specified dimension. Make sure to use `keepdim=True` to keep the channel dimension, rather than squishing it!\n",
    ">```python\n",
    ">x_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    ">mean1 = x_tensor.mean(dim=1, keepdim=False)\n",
    ">print(f\"mean1: {mean1}, shape: {mean1.shape}\")\n",
    "># Output:\n",
    ">tensor([2., 5.]) torch.Size([2])\n",
    ">\n",
    ">mean2 = x_tensor.mean(dim=1, keepdim=True)\n",
    ">print(f\"mean2: {mean2}, shape: {mean2.shape}\")\n",
    "># Output:\n",
    ">tensor([[2.],\n",
    ">       [5.]]) torch.Size([2, 1])\n",
    "\n",
    "4. You can use [`torch.expand`](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.expand.html) to copy the same image along 3 channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CS-nx-Y9RnUo",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transforms=None, spectrogram_transform=None, return_targets=True):\n",
    "        super().__init__()\n",
    "        self.audio_dir = audio_dir # Folder where the audio files will be located\n",
    "        self.transforms = transforms # Additional image transforms\n",
    "        self.spectrogram_transform = spectrogram_transform or torchaudio.transforms.Spectrogram(n_fft=1024, normalized=True)\n",
    "        self.return_targets = return_targets  # Whether to return targets or not when __get_item__ is called\n",
    "        try:\n",
    "            # Get file paths of all .wav files in the provided directory\n",
    "            self.file_paths = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing files at {audio_dir}: {e}\")\n",
    "            self.file_paths = []\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: 1. Return the number of samples in the dataset\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the file name by index\n",
    "        file_name = self.file_paths[index]\n",
    "\n",
    "        # Construct the path to the requested audio file\n",
    "        filepath = os.path.join(self.audio_dir, file_name)\n",
    "\n",
    "        # TODO: 2. Extract the target class_id from the file path if return_targets is True\n",
    "        # Hint: Files are stored in the format [freesoundID]-[classID]-[occurrenceID]-[sliceID].wav\n",
    "        # Don't forget to cast the class_id to dtype=torch.long for loss functions!\n",
    "        if self.return_targets:\n",
    "            target = ...\n",
    "\n",
    "        try:\n",
    "            # TODO: 3. Load the audio file located at `filepath`\n",
    "            waveform, sample_rate = ...\n",
    "            # TODO: 4. Convert stereo to mono by averaging channels.\n",
    "            waveform = ...\n",
    "\n",
    "            # TODO: 5. Generate a spectrogram of the waveform. Expected shape: (1, H, W)\n",
    "            spec = ...\n",
    "\n",
    "            # TODO: 6. Copy the spectrogram into 3 channels to get shape: (3, H, W)\n",
    "            spec = ...\n",
    "\n",
    "            # TODO: 7. Apply transformations\n",
    "            if self.transforms:\n",
    "                spec = ...\n",
    "\n",
    "            if self.return_targets:\n",
    "                return spec, target\n",
    "            else:\n",
    "                return spec\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {filepath}: {e}')\n",
    "            if self.return_targets:\n",
    "                return None, None\n",
    "            else:\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ap6aPOOMBjvW"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5b: Spectrogram DataLoaders\n",
    "\n",
    "Using the SpectrogramDataset we just defined, let's prepare our training and validation dataloaders.\n",
    "\n",
    "**Task:**\n",
    "1. Define a transform using `torchvision.transforms.Resize` that resizes images to (224, 224).\n",
    "    1. Most computer vision models (like the ConvNeXt model we will use) expect input images of size `224x224`. We'll use a torchvision transform to resize our spectrograms to this shape.\n",
    "2. Instantiate a `SpectrogramDataset` using `fold1` as your audio directory. Pass in the path to your `fold1` directory (which should be located under `/content/drive/MyDrive/cs189/hw/hw4/data/fold1` if you're on Colab, or wherever your UrbanSound8k data was unzipped to locally).\n",
    "3. Split your `SpectrogramDataset` instance into training and validation sets using scikit-learn's `train_test_split` with a `test_size` of 0.2.\n",
    "    1. Make sure to set `random_state=SEED` and `shuffle=True`!\n",
    "4. Create a training DataLoader and a testing DataLoader.\n",
    "    1. Use `batch_size=32`.\n",
    "    2. Set `shuffle=True` for the training DataLoader and `shuffle=False` for the testing DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dJkZM930WUf3",
    "outputId": "262a8f53-ffa1-42e0-d4de-f28fc3961ea4",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Define a transform to resize images to (224, 224)\n",
    "resize_transform = ...\n",
    "\n",
    "# TODO: Instantiate a SpectrogramDataset using the folder fold1\n",
    "spectrogram_dataset = ...\n",
    "\n",
    "# TODO: Create a 0.8 training and 0.2 test split\n",
    "spectrogram_train_dataset, spectrogram_test_dataset = ...\n",
    "\n",
    "# TODO: Create training and testing dataloaders\n",
    "spectrogram_train_dataloader = ...\n",
    "spectrogram_test_dataloader = ...\n",
    "\n",
    "# Printing out the shapes of data and targets in the first batch!\n",
    "batch = next(iter(spectrogram_train_dataloader))\n",
    "\n",
    "data, targets = batch\n",
    "print(f\"Shape of 1 batch of data: {data.shape}\")\n",
    "print(f\"Shape of 1 batch of targets: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oN75zvQGHGG7"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5c: Write a Training Loop for the Image Classifier\n",
    "\n",
    "Before we train our image classifiers, we need to write a training loop! This training loop will be very similar to the one we wrote for the DNA Classifier earlier, but it will be slightly different because of nuances in the input data: for the DNA Classifier, the model expected an `input_ids` and an `attention_mask` as its input. For our image classifier, we simply need to pass in the image as the input.\n",
    "\n",
    "**Task:** Complete the loop to train and evaluate an image classifier.\n",
    "\n",
    "For each batch inside the training loop:\n",
    "1. Move the training input data and targets to the specified device. Cast the inputs as `dtype=torch.float` (to match the dtype of the model's weights) and the targets as `dtype=torch.long` (expected data type for loss function).\n",
    "2. Reset the optimizer gradients to zero so gradients from the previous batch don't accumulate.\n",
    "3. Run a forward pass: Pass the inputs `x` through the model to produce the model's output `y_hat`.\n",
    "4. Calculate the loss between the model's output and the true labels using the `criterion` (loss function).\n",
    "5. Compute the gradients for the model's parameters by backpropagating from the loss.\n",
    "6. Update the model parameters by taking one step of the optimizer.\n",
    "7. Track training statistics.\n",
    "    1. Add the current loss to the running total in `train_loss`.\n",
    "    2. Count how many samples the model correctly predicted, and add it to `train_correct`.\n",
    "\n",
    "For each batch inside the validation loop:\n",
    "\n",
    "8. Move the validation input data and targets to the device. Cast the inputs as `dtype=torch.float` (to match the dtype of the model's weights) and the targets as `dtype=torch.long` (expected data type for loss function).\n",
    "9. Do a forward pass through the model to get predictions.\n",
    "10. Compute the loss on the validation data\n",
    "11. Track validation metrics\n",
    "    1. Calculate loss and track the number of correct validation predictions.\n",
    "\n",
    "**Hints:**\n",
    "* Make sure you pass the model outputs and the true targets to the criterion in the right order (model outputs first, then true targets)!\n",
    "* To get the predicted class when calculuating the number of correct predictions, you can use `torch.max` and specify which dimension you want to find the max over! `torch.max` will return two values: the max, and the argmax (i.e. the index where the max occurs). What is the connection betweeh the argmax and the predicted class?\n",
    "* Since `train_loss` and `train_correct` are floats and integers respectively, you can't directly add a tensor to them. To add a 1D scalar tensor to a float/integer in Python, you need to call `.item()` on the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6mmzrRbLTJ8",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_image_classifier(model, optimizer, criterion, device, num_epochs, train_dataloader, val_dataloader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: the model to train\n",
    "        optimizer: the optimizer to use\n",
    "        criterion: the loss function to use\n",
    "        num_epochs: the number of epochs to train for\n",
    "        train_dataloader: the dataloader for the training set\n",
    "        val_dataloader: the dataloader for the validation set\n",
    "    Returns:\n",
    "        train_losses: a list of training losses for each epoch\n",
    "        val_losses: a list of validation losses for each epoch\n",
    "        train_accuracies: a list of training accuracies for each epoch\n",
    "        val_accuracies: a list of validation accuracies for each epoch\n",
    "    \"\"\"\n",
    "    # === SETUP ===\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store metrics across epochs\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # === EPOCH LOOP ===\n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRAINING PHASE ===\n",
    "        model.train() # Set model to training mode\n",
    "\n",
    "        # Initialize metrics for this epoch\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        # === INNER LOOP (iterate over training batches) ===\n",
    "        for batch in train_dataloader:\n",
    "            # TODO 1: Move the data and targets to device and cast them to the appropriate dtypes\n",
    "            x, y = batch\n",
    "            x, y = ...\n",
    "\n",
    "            # TODO 2: Reset gradients\n",
    "            ...\n",
    "\n",
    "            # TODO 3: Forward pass: pass inputs to model\n",
    "            y_hat = ...\n",
    "\n",
    "            # TODO 4: Compute loss\n",
    "            loss = ...\n",
    "\n",
    "            # TODO 5: Backward pass/compute gradients\n",
    "            ...\n",
    "\n",
    "            # TODO 6: Update parameters\n",
    "            ...\n",
    "\n",
    "            # TODO 7: Track training metrics for this epoch\n",
    "            ...\n",
    "            _, preds = ...\n",
    "            ...\n",
    "\n",
    "        # === END OF INNER LOOP ===\n",
    "\n",
    "        # Compute average training metrics for the epoch\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc = train_correct / len(train_dataloader.dataset)\n",
    "\n",
    "        # Append this epoch's training metrics to history\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Training loss = {train_loss}\\tTrain accuracy = {train_acc}\")\n",
    "\n",
    "        # === END OF TRAINING PHASE ===\n",
    "\n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval() # set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # === INNER LOOP (iterate over validation batches) ===\n",
    "            for batch in val_dataloader:\n",
    "                # TODO 8: Move the data and targets to device and cast them to the appropriate dtypes\n",
    "                x, y = batch\n",
    "                x, y = ...\n",
    "\n",
    "                # TODO 9: Forward pass only\n",
    "                y_hat = ...\n",
    "\n",
    "                # TODO 10: Compute loss\n",
    "                loss = ...\n",
    "\n",
    "                # TODO 11: Track validation metrics\n",
    "                ...\n",
    "                _, preds = ...\n",
    "                ...\n",
    "\n",
    "            # === END OF INNER LOOP ===\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc = val_correct / len(val_dataloader.dataset)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Epoch {epoch + 1}: Validation loss = {val_loss}\\tValidation accuracy = {val_acc}\")\n",
    "\n",
    "        # === END OF VALIDATION PHASE ===\n",
    "\n",
    "    # === END OF EPOCH LOOP ===\n",
    "\n",
    "    print(f\"=\" * 20 + \" Final Metrics \" + \"=\" * 20)\n",
    "    print(f\"Final training loss: {train_losses[-1]:.5f}\\tFinal training accuracy = {train_accuracies[-1]:.5f}\")\n",
    "    print(f\"Final validation loss: {val_losses[-1]:.5f}\\tFinal validation accuracy = {val_accuracies[-1]:.5f}\")\n",
    "    print(f\"=\" * 55)\n",
    "\n",
    "    # Return history\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mHVWNxk15wGJ"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Introducing ConvNeXt\n",
    "ConvNeXt is a modern convolutional neural network architecture designed to combine the strengths of traditional CNNs with design principles inspired by Transformers. Introduced in 2022 by Liu et al. in the paper [\"A ConvNet for the 2020s\"](https://arxiv.org/abs/2201.03545), it proved that pure convolutional neural networks could still perform favorably with modern transformer-based vision models! ConvNeXt updates classical CNN components (like ResNet) with design choices inspired by Vision Transformers. It was trained on the large-scale ImageNet dataset with 1000 classes, making it a powerful backbone for image classification tasks.\n",
    "\n",
    "### ConvNeXt architecture\n",
    "* **Residual connections**: Like ResNets, ConvNeXt uses skip connections to help gradients flow and enable training of very deep networks.\n",
    "* **Layer normalization**: It replaces batch normalization with layer normalization, a technique popularized by Transformers, which can improve training stability.\n",
    "* **Strided convolutions to create patches**: ConvNeXt uses a `4√ó4` convolution with stride 4 at the first layer to slice the input image into non-overlapping patches. This is similar to how Vision Transformers process patches of images to create patch embeddings.\n",
    "* **Large receptive fields**: At later layers, ConvNeXt uses large convolution kernels. The larger convolution kernels have larger receptive fields, allowing the model to capture global context.\n",
    "    * While Transformers rely on self-attention to model relationships between all parts of the input, ConvNeXt achieves similar global context awareness through large convolution kernels.\n",
    "* **Depthwise convolutions**: ConvNeXt replaces some regular convolutions with depthwise convolutions. Depthwise convolutions apply a separate convolutional filter for each channel. As a result, information is not mixed between channels.\n",
    "* **Inverted Bottleneck Block**: Inspired by feed-forward networks in Transformers, ConvNeXt has blocks where the channel count is increased to learn many different representations of the hidden outputs, before being shrunk back down to fewer channels.\n",
    "* **Fewer activations and norms per block**: Like Transformers, ConvNeXt simplifies blocks with fewer activations (GELU instead of ReLU) and fewer normalization layers, learning that this actually improves results.\n",
    "* **Advanced data augmentation and regularization**: ConvNeXt incorporates contemporary augmentation techniques like Mixup, CutMix, and regularization borrowed from ViTs for further improvements.\n",
    "\n",
    "ConvNeXt can be seen as a bridge between CNNs and Transformers, combining the efficiency and inductive biases of convolutions with some of the design insights from Transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54RWH5bHJyoD"
   },
   "source": [
    "## Question 5d: Adapting ConvNeXt for Sound Classification\n",
    "\n",
    "ConvNeXt was trained on the ImageNet dataset, which has 1000 classes. As a result, the final linear layer in the ConvNeXt's classifer has an output dimension of 1000: for each training point $x_i$ in our batch, the ConvNeXt will output a 1000-dimensional vector containing the probabilities that $x_i$ belongs to each of the 1000 classes. Our spectrogram dataset has 10 classes (air conditioner, car horn, children playing, dog bark, drilling, engine idling, gun shot, jackhammer, siren, street music). So to modify the ConvNeXt for our new sound classification task, we need to replace the final classifer layer with one that has an output dimension of 10 to match the number of classes we have.\n",
    "\n",
    "When working with PyTorch models like ConvNeXt, it's important to understand how modules and layers are organized and accessed:\n",
    "* Most models have named attributes for their main components. For ConvNeXt, the classifier head is stored in the `model.classifier` attribute. This is typically a `nn.Sequential` container holding several layers.\n",
    "* You can access individual layers inside a sequential module by indexing. For example, `model.classifier[-1]` gets the last layer in the classifier sequence (which is usually the final linear layer responsible for outputting class scores).\n",
    "* To get the `in_features` and `out_features` dimension of a linear layer, you can access its `.in_features` attribute.\n",
    "* Lastly, when replacing layers, you can directly assign a new layer to a position in the sequence. For instance, `model.classifier[-1] = nn.Linear(...)` replaces the last layer with a new linear layer that outputs the desired number of classes.\n",
    "\n",
    "**Task:** Implement `replace_final_convnext_linear_layer`, which repalces the final linear layer of the ConvNeXt's classifier module with a new linear layer that outputs the specified number of classes.\n",
    "\n",
    "**Hints:**\n",
    "* Use the guidelines above to access the final linear layer of the model's classifier module. For the new linear layer we will replace the final linear layer with, what should the `in_features` parameter be and what should the `out_features` paramter be so that our model outputs the right number of class predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmFLX0Xy3KzR",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def replace_final_convnext_linear_layer(model, num_classes=10):\n",
    "    # TODO: Access the classifier module of the model\n",
    "    classifier = ...\n",
    "\n",
    "    # TODO: Get the input dimensions of the classifier's last linear layer\n",
    "    in_features = ...\n",
    "\n",
    "    # TODO: repalce the model's last linear layer with a new linear layer\n",
    "    ...\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jwJOf7N-y-JF"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Pre-training and Fine-tuning\n",
    "\n",
    "In part 1 of this homework, we trained our own models from the ground up. In modern machine learning, fine-tuning pretrained models has become a foundational technique that offers significant advantages over training models from scratch. We can leverage models that have already learned rich feature representations from massive datasets, and then adapt them efficiently to specific tasks with limited data and computational resources! Earlier, we adapted DNABERT (pretrained to encode DNA kmers) by fine-tuning it to classify DNA sequences. Now, we'll fine-tune ConvNeXt, which was trained on ImageNet-1k, for spectrogram classification.\n",
    "\n",
    "### A quick refresher on backbones vs. classification heads\n",
    "*  Backbone: The main part of the model that extracts features from input images (e.g., all convolutional layers).\n",
    "* Classification Head: The final layers that take backbone features and output class predictions (e.g., a linear layer mapping to class scores).\n",
    "\n",
    "### Loading pretrained models in PyTorch\n",
    "PyTorch makes it easy to load pretrained models for computer vision tasks. For example, you can load ResNet50 with pretrained weights using:\n",
    ">```python\n",
    ">from torchvision.models import resnet50, ResNet50_Weights\n",
    ">\n",
    "> # Pretrained on ImageNet\n",
    "> model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    ">\n",
    "> # Best available weights (currently alias for IMAGENET1K_V2)\n",
    "> # Note that these weights may change across versions\n",
    "> model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    ">\n",
    "># Strings are also supported\n",
    "> model = resnet50(weights=\"IMAGENET1K_V2\")\n",
    ">```\n",
    "\n",
    "If we want only the ResNet50 model architecture itself, we can load it with randomly initialized weights:\n",
    ">```python\n",
    "> # No weights - random initialization\n",
    "> model = resnet50(weights=None)\n",
    ">```\n",
    "\n",
    "You can read more about working with [PyTorch's pretrained models in the documentation here](https://docs.pytorch.org/vision/main/models.html)!\n",
    "\n",
    "\n",
    "### Fine-tuning paradigms\n",
    "When adapting a pretrained model to a new task, you have several options:\n",
    "\n",
    "#### 1. Training from Scratch (Uninitialized Weights)\n",
    "* Load the model with randomly initailized weights by specifying`weights=None`.\n",
    "* The model learns everything from your dataset, but may require more data and training time.\n",
    "\n",
    "#### 2. Pretrained Weights with Frozen Backbone:\n",
    "* Load the model with pretrained weights.\n",
    "* \"Freeze\" the backbone by setting `requires_grad=False` for its parameters. This tells PyTorch not to track gradients for any weights in the backbone. When we perform the optimizer step after backpropagation, none of the backbone's weights will get updated. Only the classification head is trainable.\n",
    "* The backbone acts as a fixed feature extractor; only the head adapts to your new classes.\n",
    "* Useful when you have limited data or want to avoid overfitting.\n",
    "\n",
    "#### 3. Pretrained Weights with All Layers Trainable (Full Fine-tuning):\n",
    "* Load the model with pretrained weights.\n",
    "* All layers (backbone and head) are trainable (all their parameters have `requires_grad=True`).\n",
    "* The model can adapt all its features to your new dataset, often leading to better performance if you have enough data. Even though we are updating all the weights in the model during training, our model can train faster or reach better metrics faster becuase the backbone already has some feature extraction ability, even if it was previously pretrained on a totally different dataset.\n",
    "\n",
    "### How to Freeze/Unfreeze Layers\n",
    "You can iterate through the parameters in a model using either `parameters` or `named_parameters`.\n",
    "\n",
    "To freeze all layers except for the classifier:\n",
    ">```python\n",
    ">for name, param in frozen_backbone.named_parameters():\n",
    ">    if \"classifier\" not in name: # Freeze any non-classifier layers\n",
    ">        param.requires_grad = False\n",
    ">```\n",
    "\n",
    "To unfreeze (train) all layers:\n",
    ">```python\n",
    "> for param in model.parameters():\n",
    ">   param.requires_grad = True\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FtlOzCEPIzh"
   },
   "source": [
    "## Question 5e: Train a ConvNeXt from Scratch\n",
    "\n",
    "First, we will load a ConvNeXt model with randomly initialized weights (no pretraining), adapt it for our sound classification problem, and train it on our spectrogram dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Instantiate ConvNeXt with no pretrained weights:\n",
    "    1. Use `convnext_base(weights=None)` to create a model with random weights.\n",
    "2. Use the `replace_final_convnext_linear_layer` to replace the final classification layer in the ConvNeXt.\n",
    "3. Initialize an `AdamW` optimizer and pass in the model's parameters.\n",
    "    1. Play around with different learning rates to find the best learning rate! Track if the training metrics being printed every epoch are improving meaningfully to see if your learning rate is appropriate. We recommend starting with a learning rate in the range `1e-4` to `1e-3`, and modifying the learning rate by factors of 2x (e.g. to `2e-4`, then `4e-4`) or 10x (e.g. to `1e-5` or `1e-3`) to see if it yields improved performance.\n",
    "4. Initialize `CrossEntropyLoss` as the loss function.\n",
    "5. Train the model for 5 epochs on  `spectrogram_train_dataloader` and `spectrogram_test_dataloader` using the `train_image_classifier` function.\n",
    "6. Use the helper function `plot_metrics` from question 4h to visualize the training loss, training accuracy, validation loss, and validation accuracy curves over all the epochs.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "* We have 10 possible sounds to classify a spectrogram as. Think about how this relates to the output dimension our final clasifer linear layer should have.\n",
    "* Explore a few different learning rates for your optimizer and see how that affects the training metrics. Use the best learning rate you find to train your model for your visualization.\n",
    "\n",
    "Your model should reach **> 0.20** training accuracy after 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1560
    },
    "id": "MyWgFegEPNeA",
    "outputId": "e598527e-cd64-46e4-96ae-7accef7a0d73",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Load a ConvNeXt model with uninitialized weights\n",
    "uninitialized_convnext = ...\n",
    "\n",
    "# TODO: Replace the final classification layer of the ConvNeXt\n",
    "uninitialized_convnext = ...\n",
    "\n",
    "# TODO: Initialize the optimizer\n",
    "optimizer = ...\n",
    "\n",
    "# TODO: Define the loss function\n",
    "criterion = ...\n",
    "\n",
    "# TODO: Train your model for 5 epochs\n",
    "unintialized_train_losses, unintialized_val_losses, unintialized_train_accuracies, unintialized_val_accuracies = ...\n",
    "\n",
    "# TODO: Plot the metrics\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LgW6Zt-cRA22"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5f: Train a ConvNeXt with a Frozen Backbone\n",
    "\n",
    "Now, you'll adapt and train a ConvNeXt model using pretrained weights, but only the classifier head will be updated‚Äîthe backbone will be \"frozen\" and act as a fixed feature extractor.\n",
    "\n",
    "**Tasks:**\n",
    "1. Instantiate ConvNeXt with pretrained weights:\n",
    "    1. Use `weights=ConvNeXt_Base_Weights.IMAGENET1K_V1` to load a model with weights trained on ImageNet.\n",
    "2. Use the `replace_final_convnext_linear_layer` to replace the final classification layer in the ConvNeXt\n",
    "3. Freeze the backbone. By default, all the parameters in all the layers of the model will be trainable. Set `requires_grad=False` for all model parameters except those in the classifier.\n",
    "4. Initialize an `AdamW` optimizer and pass in the model's parameters.\n",
    "    1. Note: Even if you pass in all the model's parameters to the optimizer, it wil only update those that have `requires_grad` set to `True`.\n",
    "    2. Play around with different learning rates to find the best learning rate! Track if the training metrics being printed every epoch are improving meaningfully to see if your learning rate is appropriate. We recommend starting with a learning rate in the range `1e-4` to `1e-3`, and modifying the learning rate by factors of 2x (e.g. to `2e-4`, then `4e-4`) or 10x (e.g. to `1e-5` or `1e-3`) to see if it yields improved performance.\n",
    "5. Initialize `CrossEntropyLoss` as the loss function.\n",
    "6. Train the model for 5 epochs on  `spectrogram_train_dataloader` and `spectrogram_test_dataloader` using the `train_image_classifier` function.\n",
    "7. Use `plot_metrics` to visualize the training loss, training accuracy, validation loss, and validation accuracy curves over all the epochs.\n",
    "\n",
    "**Hints:**\n",
    "* `named_parameters()` produces an iterator that returns a tuple of the form (name, parameter) for each parameter in the model. How can you use the name to identify which parameters you need to freeze?\n",
    "\n",
    "Your model should reach **> 0.30** training accuracy after 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1595
    },
    "id": "Ni1UwJhKp22y",
    "outputId": "3f3e6f80-e761-4d10-a0eb-167e40b671e6",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Load a ConvNeXt model with pretrained weights\n",
    "frozen_backbone = convnext_base(weights=ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# TODO: Replace the final classification layer of the ConvNeXt\n",
    "frozen_backbone = ...\n",
    "\n",
    "# TODO: Freeze the ConvNeXt's backbone\n",
    "...\n",
    "\n",
    "# TODO: Initialize the optimizer\n",
    "optimizer = ...\n",
    "\n",
    "# TODO: Define the loss function\n",
    "criterion = ...\n",
    "\n",
    "# TODO: Train your model for 5 epochs\n",
    "frozen_bb_train_losses, frozen_bb_val_losses, frozen_bb_train_accuracies, frozen_bb_val_accuracies = ...\n",
    "\n",
    "# TODO: Plot the metrics\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wMXr8jSnumAm"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5g: Train a ConvNeXt with All Layers Unfrozen\n",
    "\n",
    "Lastly, we'll use ConvNeXt pretrained on ImageNet, but we'll allow all model layers‚Äîincluding the backbone and the classification head‚Äîto update during training. This is called full finetuning and usually achieves the best adaptation to our data when we have enough examples.\n",
    "\n",
    "**Tasks:**\n",
    "1. Instantiate ConvNeXt with pretrained weights:\n",
    "    1. Use `weights=ConvNeXt_Base_Weights.IMAGENET1K_V1` to load a model with weights trained on ImageNet.\n",
    "2. Use the `replace_final_convnext_linear_layer` to replace the final classification layer in the ConvNeXt.\n",
    "3. Ensure all layers are trainable. By default, all parameters in the model have `requires_grad=True`, but to be safe, loop through all the parameters again and explicitly set them to be trainable.\n",
    "4. Initialize an `AdamW` optimizer and pass in the model's parameters.\n",
    "    1. Play around with different learning rates to find the best learning rate! Track if the training metrics being printed every epoch are improving meaningfully to see if your learning rate is appropriate. We recommend starting with a learning rate in the range `1e-4` to `1e-3`, and modifying the learning rate by factors of 2x (e.g. to `2e-4`, then `4e-4`) or 10x (e.g. to `1e-5` or `1e-3`) to see if it yields improved performance.\n",
    "5. Initialize `CrossEntropyLoss` as the loss function.\n",
    "6. Train the model for 5 epochs on  `spectrogram_train_dataloader` and `spectrogram_test_dataloader` using the `train_image_classifier` function.\n",
    "7. Use `plot_metrics` to visualize the training loss, training accuracy, validation loss, and validation accuracy curves over all the epochs.\n",
    "\n",
    "**Hints:**\n",
    "* You can confirm if all of the model's parameters are trainable by printing out `parameter.requires_grad()` for each parameter in the model.\n",
    "\n",
    "Your model should reach **> 0.90** training accuracy after 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1560
    },
    "id": "yuSvGQQCtLip",
    "outputId": "37dc1134-5733-4679-a276-5d89ee68ebbb",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Load a ConvNeXt model with pretrained weights\n",
    "unfrozen_convnext = convnext_base(weights=ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# TODO: Replace the final classification layer of the ConvNeXt\n",
    "unfrozen_convnext = ...\n",
    "\n",
    "# TODO: Make sure all the parameters are trainable\n",
    "...\n",
    "\n",
    "# TODO: Initialize the optimizer\n",
    "optimizer = ...\n",
    "\n",
    "# TODO: Define the loss function\n",
    "criterion = ...\n",
    "\n",
    "# TODO: Train your model for 5 epochs\n",
    "unfrozen_train_losses, unfrozen_val_losses, unfrozen_train_accuracies, unfrozen_val_accuracies = ...\n",
    "\n",
    "# TODO: Plot the metrics\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SPbFoI4LWXFD"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5h: Comparing Results of Different Fine-tuning Paradigms\n",
    "\n",
    "Which of the 3 fine-tuning approaches was the most effective? Why? Can you think of some disadvantages of this approach, and some scenarios where we might want to use the other 2 approaches instead?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BPgywRup09cm"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5i: Listening to our Classifier's Predictions!\n",
    "\n",
    "Now that your ConvNeXt is trained, let‚Äôs have some fun. We'll listen to a sound and compare it to what our model classifies the sound as!\n",
    "\n",
    "**Task:**\n",
    "1. Choose an audio file to listen to, and save its file path into the variable `audio_file`.\n",
    "2. Load the `.wav` file using `torchaudio.load`.\n",
    "3. If the audio is stereo, average the channels to create a single mono waveform.\n",
    "4. Generate a spectrogram of the audio's waveform using `torchaudio`'s `spectrogram` transform.\n",
    "5. Resize the spectrogram into a (224 x 224) image.\n",
    "6. Expand the spectrogram's number of channels from 1 channel to 3 channels for model compatibility.\n",
    "7. Add a batch dimension (so the shape of the spectrogram is `(1, 3, 224, 224)`).\n",
    "8. Move the input to the correct device and cast it as `torch.float`.\n",
    "9. Pass the spectrogram to your trained ConvNeXt (pick any of the 3 you trained earlier!).\n",
    "10. Get the model's prediction by finding the class with the highest score.\n",
    "11. Look up the class label corresponding to the ConvNeXt's predicted class using the `class_id_to_sound` dictionary.\n",
    "12. Parse the audio file's name to get the true class ID and look up the class label corresponding to the true class ID using the `class_id_to_sound` dictionary.\n",
    "\n",
    "Listen to the sound and see if you would make the same prediction as your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "HLqmiYa1tlkx",
    "outputId": "f5723b8d-6efa-4ca9-a4cb-0c50a892ebbb",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO 1: Pick an audio file to listen to and save it to the `audio_file` variable\n",
    "audio_file = '/content/drive/MyDrive/cs189/hw/hw4/data/fold1/101415-3-0-2.wav'\n",
    "if IS_COLAB:\n",
    "    from IPython.display import Audio, display\n",
    "    display(Audio(audio_file, autoplay=False))\n",
    "else:\n",
    "    import pygame\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(audio_file)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "try:\n",
    "    # TODO 2: Load the .wav audio file\n",
    "    waveform, sample_rate = ...\n",
    "\n",
    "    # TODO 3: Average the channels to create a single mono channel\n",
    "    waveform = ...\n",
    "\n",
    "    # TODO 4: Generate a spectrogram\n",
    "    spec = ...\n",
    "\n",
    "    # TODO 5: Resize the spectrogram into the shape (1 x 224 x 224)\n",
    "    spec = ...\n",
    "\n",
    "    # TODO 6: Repeat the spectrogram 3 times to have 3 channels\n",
    "    spec = ...\n",
    "\n",
    "    # TODO 7: Add a batch dimension\n",
    "    spec = ...\n",
    "\n",
    "    # TODO 8: Move the input to the right device and cast it to the right dtype\n",
    "    spec = ...\n",
    "\n",
    "    # TODO 9: Get the model's outputs\n",
    "    y_hat = ...\n",
    "\n",
    "    # TODO 10: find the class with the highest output score\n",
    "    _, pred = ...\n",
    "\n",
    "    # TODO 11: Look up the class label of the model's prediction\n",
    "    predicted_class = ...\n",
    "\n",
    "    # TODO 12: Parse the audio file's name to get the true class ID and the true class label\n",
    "    true_class_id = ...\n",
    "    true_class = ...\n",
    "\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    print(f\"True class: {true_class}\")\n",
    "except Exception as e:\n",
    "    print(f'Error processing {audio_file}: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8IA7WorJVf3R"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5j: Sound Classification Kaggle Competition\n",
    "\n",
    "Just like the DNA classification competition, we can take the audio classification model we just trained and make predictions for our 2nd class Kaggle competition! Your homework folder should include two files: `urbansound8k_test.zip` and `urbansound8k_test.csv`. The zip file contains 175 audio files in .wav format, and the CSV file contains the mapping of IDs to filenames. In the cell below, we've written a helper function for you that unzips the test audio files, loads them, makes predictions for each audio sample, and saves the predictions to a csv file called `urbansound8k_test_predictions.csv`.\n",
    "\n",
    "You should then submit your csv file to the Kaggle competition: [Fa25 CS189 HW4 - UrbanSound8k](https://www.kaggle.com/competitions/fa-25-cs-189-hw-4-urban-sounds-8-k/overview)\n",
    "\n",
    "**Expected Output:**\n",
    "* Your `urbansound8k_test_predictions.csv` file should have 2 columns:\n",
    "    * `ID`: ID of the audio sample from the provided test set\n",
    "    * `class`: the predicted class of the audio sample (0-9, corresponding to the 10 UrbanSound8K classes: `air_conditioner`, `car_horn`, `children_playing`, `dog_bark`, `drilling`, `engine_idling`, `gun_shot`, `jackhammer`, `siren`, `street_music`).\n",
    "\n",
    "**Task:**\n",
    "* Train an instance of a sound ConvNeXt classifier. You can try out different fine-tuning approaches, such as freezing or unfreezing certain layers beyond the ones we tried earlier in questions 5e, 5f, and 5g!\n",
    "* Call `generate_urbansound8k_test_predictions` with your model to generate a `urbansound8k_test_predictions.csv` file.\n",
    "* Submit your predictions to the UrbanSound8k classification Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z340oZ1DQ8US"
   },
   "outputs": [],
   "source": [
    "!unzip -q urbansound8k_test.zip -d urbansound8k_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "an3Dn8U2Q9I-",
    "outputId": "343ab71d-ac48-418b-9e64-b62e624a00ff"
   },
   "outputs": [],
   "source": [
    "def generate_urbansound8k_test_predictions(model):\n",
    "    \"\"\"\n",
    "    Generates predictions for the UrbanSound8K test set using the specified model.\n",
    "\n",
    "    Loads the test metadata from 'urbansound8k_test.csv', applies necessary transforms\n",
    "    to each audio sample in the 'urbansound8k_test' directory, and creates a test dataset.\n",
    "    Runs the model in evaluation mode to predict class labels for each test spectrogram.\n",
    "    Predictions are saved to 'urbansound8k_test_predictions.csv' with columns 'ID' and 'class',\n",
    "    where 'ID' is taken from the test CSV.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained PyTorch model for UrbanSound8K classification.\n",
    "\n",
    "    Outputs:\n",
    "        urbansound8k_test_predictions.csv: CSV file with index column 'ID' and a 'class' column\n",
    "        containing the predicted class for each audio sample in the test set.\n",
    "    \"\"\"\n",
    "    # Load the UrbanSound8k test CSV with IDs\n",
    "    test_df = pd.read_csv('urbansound8k_test.csv')\n",
    "\n",
    "    # Define the resize transform (same as used for training)\n",
    "    resize_transform = torchvision.transforms.Resize(size=[224, 224])\n",
    "\n",
    "    # Create test dataset with return_targets=False\n",
    "    test_dataset = SpectrogramDataset(\n",
    "        audio_dir='urbansound8k_test',\n",
    "        return_targets=False,\n",
    "        transforms=resize_transform\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)): # Iterate through all samples in our Dataset\n",
    "            spec = test_dataset[i]\n",
    "\n",
    "            if spec is None:\n",
    "                predictions.append(-1)\n",
    "                continue\n",
    "\n",
    "            # Add batch dimension and move to device\n",
    "            spec = spec.unsqueeze(0).to(device) # shape: (1, 224, 224)\n",
    "\n",
    "            # Make prediction\n",
    "            output = model(spec)\n",
    "            pred_class = torch.argmax(output, dim=1).item()\n",
    "            predictions.append(pred_class)\n",
    "\n",
    "    # Create predictions dataframe\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ID': test_df['ID'],\n",
    "        'class': predictions\n",
    "    }).set_index('ID')\n",
    "\n",
    "    predictions_df.to_csv('urbansound8k_test_predictions.csv', index=True) # Generates a csv with 2 columns: ID and class\n",
    "    print(\"Submissions saved to urbansound8k_test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "U-Rkx86hkz1c",
    "outputId": "253b92f9-87ec-4c99-91c3-1a2958891688",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Optional TODO: Instantiate and train your best DNA classification model!\n",
    "\n",
    "# TODO: Use your best sound classifier to make a prediction\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lFTTsDXKsVyU"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Submission Checklist\n",
    "\n",
    "Please make sure you have completed the following steps before submitting your assignment.\n",
    "\n",
    "## Have you...\n",
    "- [ ] Saved your notebook as a PDF, ensuring that all the code cells, written questions (if applicable), and graphs are visible?\n",
    "- [ ] Included your **Kaggle username** and a screenshot of your **2 Kaggle scores** in your PDF submission?\n",
    "- [ ] Run the export cell in the notebook to generate a zip file containing all the code from this notebook?\n",
    "- [ ] Included an acknowledgement of any GenAI assistance you used while completing this assignment, if applicable? (See the [CS 189/289 syllabus](https://eecs189.org/fa25/syllabus/#collaboration-policy-and-academic-honesty) for full details of the course's GenAI policy and for an example acknowledgement format you can use).\n",
    "\n",
    "## Submissions\n",
    "- [ ] Submitted your test set predictions for the **Fa25 CS189 HW 4 - DNABERT** and **Fa25 CS189 HW4 - UrbanSound8k** Kaggle competitions?\n",
    "- [ ] Submitted the PDF of your notebook and Kaggle submissions to the `HW 4.2 Coding [PDF]` assignment on Gradescope?\n",
    "- [ ] Submitted the zip file of your code (generated by the export cell) and Kaggle submission code to the `HW 4.2 Coding [Code]` assignment on Gradescope?\n",
    "\n",
    "Congratulations! You have completed Homework 4 Part 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqSxZdeTsVyU"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell if you are running the notebook in Google Colab to install the necessary dependencies, this may take a few minutes\n",
    "if IS_COLAB:\n",
    "    !apt-get install -y texlive texlive-xetex pandoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a5754e4ca6141cbb5199815d6aae98f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c1bc6a70ea44774b2dea16fb2ea5942": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5a66a56316c4747813a0481b5ae3cb9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9f523bb8c0904da7ad12a5617fa73670",
      "value": "configuration_bert.py:‚Äá100%"
     }
    },
    "0d4e3b26e633429889feac61a877e8a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e302db1f748433cbb5a9114f02f9d98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f2cf6d2e6a1415894ca3528e6d3eef4",
       "IPY_MODEL_fb189504e3074bb48aec3d1f97aaf41b",
       "IPY_MODEL_b4f7b6f87b1a44179c0a8b62038b2857"
      ],
      "layout": "IPY_MODEL_84bd56739db5408b8e8bf3edb50de830"
     }
    },
    "12d33f3ae3104692af27c27a74bc308e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14d61d2d34394e20a8af019d950e4dbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1757fc6eb4ff46ddb5366cbc1f12cccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e9b42db72874dcda78f859337e7bdd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70ec29777ee04d3ba36df8607a2f11a0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6f7cde7a15204dabbfc6e434bb892e59",
      "value": "‚Äá1.45k/?‚Äá[00:00&lt;00:00,‚Äá151kB/s]"
     }
    },
    "26ab0dc1d18640e29ac782e0d57b2e91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4edf5309f0394a9897eca697de448302",
      "max": 359199902,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a100e88ae5654a4088bd58f5efac1805",
      "value": 359199902
     }
    },
    "286669320bc847efb5d46fec206e6ec7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28eddf29a5064259a335df0dd300cc90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3210ae8b7d3042fe8ed41c21e0c5b7cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "338a6b71370241dfaf433e0d5faf878b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a6daef02fd42f8b1ba7cae176ce94a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3a4abf07b34c45b78a248449a9720e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a3cc281c4e9448b8c5fd5a7c07b46bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db0d7df7ae0c478bba1d8cec2d97d968",
       "IPY_MODEL_5f5e01e9e45c467095d9e256cba61836",
       "IPY_MODEL_e7acf4d64f674e50851c60159f6b641a"
      ],
      "layout": "IPY_MODEL_6506ecc0f2e34e54bfad5d7483a3dddd"
     }
    },
    "4af89811804345c88e4ac42d997c26a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7c029f52f6e4efc98006b899336336a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_12d33f3ae3104692af27c27a74bc308e",
      "value": "‚Äá807/807‚Äá[00:00&lt;00:00,‚Äá85.5kB/s]"
     }
    },
    "4d1900b7e856419e81854b7356573b4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4edf5309f0394a9897eca697de448302": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f2cf6d2e6a1415894ca3528e6d3eef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5510bca19554eb5a56c3cb5b7e1c043",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5351780c0a5c4677b94c138776a3d648",
      "value": "dnabert_layer.py:‚Äá"
     }
    },
    "5279e23b1c844945824aed79e7bd333d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "5351780c0a5c4677b94c138776a3d648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54869bb1f41e430d962f1a1c150f867f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b726ba72821439eb731112fc115fdd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f5e01e9e45c467095d9e256cba61836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62523bc4f6c84c8a8b29eb21d0e05d45",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33a6daef02fd42f8b1ba7cae176ce94a",
      "value": 1
     }
    },
    "62523bc4f6c84c8a8b29eb21d0e05d45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "6506ecc0f2e34e54bfad5d7483a3dddd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65aa8c96eb9841ca9387e1bb2c404b32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "694bb251f8a7465abf45ede2a97e0334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71ee86ef180a4392b1770d20f724f671",
      "max": 40,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a4abf07b34c45b78a248449a9720e21",
      "value": 40
     }
    },
    "6bb9bd149fb241e39ebc23017bda5927": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_76cfc720798845e881a1e32ad03f9667",
       "IPY_MODEL_26ab0dc1d18640e29ac782e0d57b2e91",
       "IPY_MODEL_d201d1ab425741e990ddbfc331be13e4"
      ],
      "layout": "IPY_MODEL_5b726ba72821439eb731112fc115fdd7"
     }
    },
    "6f7cde7a15204dabbfc6e434bb892e59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70ec29777ee04d3ba36df8607a2f11a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71ee86ef180a4392b1770d20f724f671": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d9d78819074a1b91e1dc91a9274d91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76cfc720798845e881a1e32ad03f9667": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae1aa712b3af42b8aefedb9e58454342",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0d4e3b26e633429889feac61a877e8a6",
      "value": "pytorch_model.bin:‚Äá100%"
     }
    },
    "7afd46d960f9484699d00fc37292afab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84bd56739db5408b8e8bf3edb50de830": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869e85af581747b6b9a3bd7e5cc897fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c25559e68ff4717b3bc7277898a3038": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8cf5c028e7c244f59ac63f464eec4073": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dcfb66c68c04abcad3225d9cbc6d0a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cf5c028e7c244f59ac63f464eec4073",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0a5754e4ca6141cbb5199815d6aae98f",
      "value": "‚Äá112/112‚Äá[00:00&lt;00:00,‚Äá15.2kB/s]"
     }
    },
    "8f0473e6c6984dd38dacf9f686a0ebcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "91a573cc20264230a27aa49962114870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "961d17e407e449d291d61a17e251d1df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b535afc4ced147b4a4a075d2d29d6c9d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c8f712be35b94e268e535f23bdc60935",
      "value": "config.json:‚Äá"
     }
    },
    "967fb3d27fe34bd9b3a4287a571069d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_961d17e407e449d291d61a17e251d1df",
       "IPY_MODEL_d433e50b0a884204a1f197423a775183",
       "IPY_MODEL_1e9b42db72874dcda78f859337e7bdd8"
      ],
      "layout": "IPY_MODEL_28eddf29a5064259a335df0dd300cc90"
     }
    },
    "983e6390aeb4472bbeea166a09af9015": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9f523bb8c0904da7ad12a5617fa73670": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0e550d57b704f42aecf35714c25518e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a100e88ae5654a4088bd58f5efac1805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a543efcb4bce4b848626be037f7a1b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d1900b7e856419e81854b7356573b4b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8c25559e68ff4717b3bc7277898a3038",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "a8d032247800426a81f6b10374cca3d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a90e22677c9c49589481bd8f8adf616f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eaf9709e5a8b4f05a55ca702b69c1266",
       "IPY_MODEL_e961336c34634eacb2566e474f43c975",
       "IPY_MODEL_8dcfb66c68c04abcad3225d9cbc6d0a1"
      ],
      "layout": "IPY_MODEL_f6c32b1d1c844436af7080c965809aa1"
     }
    },
    "ad1cbfdfabd340fa9d6c595dae6b6c25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_338a6b71370241dfaf433e0d5faf878b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_54869bb1f41e430d962f1a1c150f867f",
      "value": "‚Äá359M/359M‚Äá[00:01&lt;00:00,‚Äá310MB/s]"
     }
    },
    "ae1aa712b3af42b8aefedb9e58454342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af843f8f6ec74644986be6c931c366a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1757fc6eb4ff46ddb5366cbc1f12cccf",
      "max": 359172492,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f0473e6c6984dd38dacf9f686a0ebcd",
      "value": 359172492
     }
    },
    "b06348be40064caa8cb71ddf848584cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc453eacacad40dc88ca7238904803e5",
      "max": 807,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65aa8c96eb9841ca9387e1bb2c404b32",
      "value": 807
     }
    },
    "b0f0596535d4413cbd3687dfcebefb6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d02894f794584720bb341194d4e11b37",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a0e550d57b704f42aecf35714c25518e",
      "value": "‚Äá40.0/40.0‚Äá[00:00&lt;00:00,‚Äá5.25kB/s]"
     }
    },
    "b4d63a29e2cf46019b94e986f5e4de05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b4f7b6f87b1a44179c0a8b62038b2857": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e172abbd9fc246819adf50427e1f1e0c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_dc64e33b1add4abfabe3c1745558c131",
      "value": "‚Äá5.44k/?‚Äá[00:00&lt;00:00,‚Äá666kB/s]"
     }
    },
    "b535afc4ced147b4a4a075d2d29d6c9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7c029f52f6e4efc98006b899336336a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc453eacacad40dc88ca7238904803e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd1dc11c3b8c4571b7fc3419e0cfd4f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a543efcb4bce4b848626be037f7a1b07",
       "IPY_MODEL_af843f8f6ec74644986be6c931c366a2",
       "IPY_MODEL_ad1cbfdfabd340fa9d6c595dae6b6c25"
      ],
      "layout": "IPY_MODEL_73d9d78819074a1b91e1dc91a9274d91"
     }
    },
    "c8f712be35b94e268e535f23bdc60935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb0094bff733408299aad830ab12d52f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d02894f794584720bb341194d4e11b37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d201d1ab425741e990ddbfc331be13e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea54e27462db4bc88afb3c1312b1f256",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fe2225bb6ee7416faa287d0161917fe0",
      "value": "‚Äá359M/359M‚Äá[00:02&lt;00:00,‚Äá208MB/s]"
     }
    },
    "d433e50b0a884204a1f197423a775183": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5279e23b1c844945824aed79e7bd333d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b4d63a29e2cf46019b94e986f5e4de05",
      "value": 1
     }
    },
    "d43546af0c8b42418367ec61f95a4b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c1bc6a70ea44774b2dea16fb2ea5942",
       "IPY_MODEL_b06348be40064caa8cb71ddf848584cd",
       "IPY_MODEL_4af89811804345c88e4ac42d997c26a3"
      ],
      "layout": "IPY_MODEL_cb0094bff733408299aad830ab12d52f"
     }
    },
    "d9254e28efad4a20853024ad45462a06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db0d7df7ae0c478bba1d8cec2d97d968": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9254e28efad4a20853024ad45462a06",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_91a573cc20264230a27aa49962114870",
      "value": "vocab.txt:‚Äá"
     }
    },
    "dc64e33b1add4abfabe3c1745558c131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de02e5cc45db4a659215ef7d251d6605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3210ae8b7d3042fe8ed41c21e0c5b7cd",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_286669320bc847efb5d46fec206e6ec7",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "e1694810a15d407b9229c0b0f021443d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de02e5cc45db4a659215ef7d251d6605",
       "IPY_MODEL_694bb251f8a7465abf45ede2a97e0334",
       "IPY_MODEL_b0f0596535d4413cbd3687dfcebefb6c"
      ],
      "layout": "IPY_MODEL_14d61d2d34394e20a8af019d950e4dbc"
     }
    },
    "e172abbd9fc246819adf50427e1f1e0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5a66a56316c4747813a0481b5ae3cb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7acf4d64f674e50851c60159f6b641a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_869e85af581747b6b9a3bd7e5cc897fc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f7ba7bc9d7934cfabdff496b1ae74629",
      "value": "‚Äá28.7k/?‚Äá[00:00&lt;00:00,‚Äá3.34MB/s]"
     }
    },
    "e961336c34634eacb2566e474f43c975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_feb68d056a294f649e449a6e83593db0",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7afd46d960f9484699d00fc37292afab",
      "value": 112
     }
    },
    "ea54e27462db4bc88afb3c1312b1f256": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eaf9709e5a8b4f05a55ca702b69c1266": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff37e8fc512c4d34a95cd99c65921581",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_eb27f3b41b2b4df0aa59cb3e628146de",
      "value": "special_tokens_map.json:‚Äá100%"
     }
    },
    "eb27f3b41b2b4df0aa59cb3e628146de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5510bca19554eb5a56c3cb5b7e1c043": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6c32b1d1c844436af7080c965809aa1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7ba7bc9d7934cfabdff496b1ae74629": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb189504e3074bb48aec3d1f97aaf41b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8d032247800426a81f6b10374cca3d6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_983e6390aeb4472bbeea166a09af9015",
      "value": 1
     }
    },
    "fe2225bb6ee7416faa287d0161917fe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "feb68d056a294f649e449a6e83593db0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff37e8fc512c4d34a95cd99c65921581": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
